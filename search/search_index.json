{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Turnkey Precision Oncology (TPO) is a scalable and integrated platform for executing precision oncology analysis workflows. TPO implements a number of NGS data analysis tasks for both DNA and RNA sequencing data, such as variant calling ( cords_somatic ) or fusion detection ( crisp_codac ). Those tasks can be composed into pipelines to implement end-to-end workflows. TPO provides functionality to integrate the multi-modal results, a schema to store data in an SQL database, and a patient-level web application to interactively explore the data. The tasks are natively executed in a standardized running environment (a docker image) on the Google Cloud Platform GCP , but can also be run locally. Structure of the documentation The documentation is broken into multiple sections covering each element listed above. Overview describes how TPO works on a high level. Setup provides instruction on installation and configuration. Usage provides detailed documentation on how to use TPO. Output documentation of TPO outputs and file formats. Development instructions on updating and extending TPO.","title":"Home"},{"location":"#introduction","text":"Turnkey Precision Oncology (TPO) is a scalable and integrated platform for executing precision oncology analysis workflows. TPO implements a number of NGS data analysis tasks for both DNA and RNA sequencing data, such as variant calling ( cords_somatic ) or fusion detection ( crisp_codac ). Those tasks can be composed into pipelines to implement end-to-end workflows. TPO provides functionality to integrate the multi-modal results, a schema to store data in an SQL database, and a patient-level web application to interactively explore the data. The tasks are natively executed in a standardized running environment (a docker image) on the Google Cloud Platform GCP , but can also be run locally.","title":"Introduction"},{"location":"#structure-of-the-documentation","text":"The documentation is broken into multiple sections covering each element listed above. Overview describes how TPO works on a high level. Setup provides instruction on installation and configuration. Usage provides detailed documentation on how to use TPO. Output documentation of TPO outputs and file formats. Development instructions on updating and extending TPO.","title":"Structure of the documentation"},{"location":"index%20-%20Copy/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"index%20-%20Copy/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"index%20-%20Copy/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"index%20-%20Copy/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"overview/","text":"Overview Glossary Task is the unit of TPO execution implementing a data processing or analysis function. Pipeline is a series of interdependent tasks together implementing an NGS analysis workflow. Local refers to the machine on which the user runs the TPO CLI. Cloud refers to the Google Cloud Platform (GCP). Remote refers generally to infrastructure (compute, storage) on the GCP. Docker Image TPO uses docker images to manage the environment and software in which all tasks are executed. GCP Image TPO uses GCP images to create disk volumes to boot configured instances or provide reference files. GCP Instance Virtual machine hosted on Google's infrastructure running a TPO GCP Image . Basic example TPO enables users to run genomics analyses on the cloud (or locally) using simple command line invocations. For example, the following command would run somatic variant calling in a TPO GCP instance. tpotask.sh -project <project.ini> cords_somatic \\ run1 -tsample tumor -nsample normal -alnt <tumor alignment> -alnn <normal alignment> Breaking the command into parts we have: $TPO_ROOT is the location of the TPO root directory as cloned from git, typically added to $PATH . tpotask.sh is the main CLI application to launch TPO tasks. <project.ini> is a configuration file that controls all aspects of how TPO works locally or in the cloud. cords_somatic is the name of the TPO somatic variant calling task that will be launched. run1 is the user-specified ID of this pipeline execution (run) and -tsample tumor -nsample normal -alnt <tumor alignment> -alnn <normal alignment> are the other parameters controlling the inputs and outputs for the chosen task cords_somatic . At the very high level, what happens after a TPO task is executed is simple: An instance is launched on GCP. The instance downloads input files from a cloud storage bucket on GCP, runs genomic analyses, uploads output results to a bucket. The instance terminates. All the necessary images are created in local using the configuration files which are then pushed to the GCP. Hence, all the task executions are actually being performed in the cloud and not in our local. One of the reasons behind it is because these tasks are very resource heavy which require powerful machines if we are to run in out local computers. The application uses docker images for tasks that need to be performed. These images are pushed to the GCR (Container Registry) which will be installed in the new instance that gets created. Once the task is completed, the output is uploaded in the GCS(Google Cloud Storage). Configuration TPO is configured through one or multiple configuration files using the INI format. Since every aspect of TPO can be configured it is helpful to use multiple configuration files each controlling a specific aspect of TPO function. Defaults are provided for a number of reference genomes.","title":"Overview"},{"location":"overview/#overview","text":"","title":"Overview"},{"location":"overview/#glossary","text":"Task is the unit of TPO execution implementing a data processing or analysis function. Pipeline is a series of interdependent tasks together implementing an NGS analysis workflow. Local refers to the machine on which the user runs the TPO CLI. Cloud refers to the Google Cloud Platform (GCP). Remote refers generally to infrastructure (compute, storage) on the GCP. Docker Image TPO uses docker images to manage the environment and software in which all tasks are executed. GCP Image TPO uses GCP images to create disk volumes to boot configured instances or provide reference files. GCP Instance Virtual machine hosted on Google's infrastructure running a TPO GCP Image .","title":"Glossary"},{"location":"overview/#basic-example","text":"TPO enables users to run genomics analyses on the cloud (or locally) using simple command line invocations. For example, the following command would run somatic variant calling in a TPO GCP instance. tpotask.sh -project <project.ini> cords_somatic \\ run1 -tsample tumor -nsample normal -alnt <tumor alignment> -alnn <normal alignment> Breaking the command into parts we have: $TPO_ROOT is the location of the TPO root directory as cloned from git, typically added to $PATH . tpotask.sh is the main CLI application to launch TPO tasks. <project.ini> is a configuration file that controls all aspects of how TPO works locally or in the cloud. cords_somatic is the name of the TPO somatic variant calling task that will be launched. run1 is the user-specified ID of this pipeline execution (run) and -tsample tumor -nsample normal -alnt <tumor alignment> -alnn <normal alignment> are the other parameters controlling the inputs and outputs for the chosen task cords_somatic . At the very high level, what happens after a TPO task is executed is simple: An instance is launched on GCP. The instance downloads input files from a cloud storage bucket on GCP, runs genomic analyses, uploads output results to a bucket. The instance terminates. All the necessary images are created in local using the configuration files which are then pushed to the GCP. Hence, all the task executions are actually being performed in the cloud and not in our local. One of the reasons behind it is because these tasks are very resource heavy which require powerful machines if we are to run in out local computers. The application uses docker images for tasks that need to be performed. These images are pushed to the GCR (Container Registry) which will be installed in the new instance that gets created. Once the task is completed, the output is uploaded in the GCS(Google Cloud Storage).","title":"Basic example"},{"location":"overview/#configuration","text":"TPO is configured through one or multiple configuration files using the INI format. Since every aspect of TPO can be configured it is helpful to use multiple configuration files each controlling a specific aspect of TPO function. Defaults are provided for a number of reference genomes.","title":"Configuration"},{"location":"usage/","text":"TPO Command Line Interface (CLI) TPO is controlled through a single executable shell script $TPO_ROOT/tpo.sh . This executable provide a large number of sub-commands for all major tasks of using TPO. Everything that can be done in TPO is done through this interface, including: Running all analysis tasks and pipelines. Developing, debugging, and updating TPO resources and references. Deploying and configuration locally and on the cloud. TPO CLI entrypoint The $TPO_ROOT/tpo.sh entrypoint, referred from now on as tpo.sh is, is a simple wrapper around a Python CLI $TPO_ROOT/tpo.py implemented using moke , see: Local Installation . The invocation is the same for all commands. $TPO_ROOT/tpo.sh [log_options] [tpo_options] command [command_options] [arguments...] [log_options] setting passed to moke controlling logging functions. [tpo_options] presets and ini files configuring TPO. [command_options] named parameters controlling a specific command [arguments] positional arguments to the command log_options -ls LS (file_a) [default: <stderr>] logging stream -ll {debug,info,subdefault,default,warn,error} (str) [default: default] logging level -lf {nltm} (str) [default: nltm] logging format` tpo_options - kv (``str``) list of key-value pairs `k::v` separated by `;` in single `'` quotes - custom (``str``) Custom ini file, overrides all other ini files - project (``str``) Project ini file, provides runtime and GCP settings - capture (``str``) Pick capture preset, provides capture baits and targets - settings (``str``) Pick settings preset, configures task settings - genome (``str``) Pick genome preset, configures reference genome","title":"TPO Command Line Interface (CLI)"},{"location":"usage/#tpo-command-line-interface-cli","text":"TPO is controlled through a single executable shell script $TPO_ROOT/tpo.sh . This executable provide a large number of sub-commands for all major tasks of using TPO. Everything that can be done in TPO is done through this interface, including: Running all analysis tasks and pipelines. Developing, debugging, and updating TPO resources and references. Deploying and configuration locally and on the cloud.","title":"TPO Command Line Interface (CLI)"},{"location":"usage/#tpo-cli-entrypoint","text":"The $TPO_ROOT/tpo.sh entrypoint, referred from now on as tpo.sh is, is a simple wrapper around a Python CLI $TPO_ROOT/tpo.py implemented using moke , see: Local Installation . The invocation is the same for all commands. $TPO_ROOT/tpo.sh [log_options] [tpo_options] command [command_options] [arguments...] [log_options] setting passed to moke controlling logging functions. [tpo_options] presets and ini files configuring TPO. [command_options] named parameters controlling a specific command [arguments] positional arguments to the command log_options -ls LS (file_a) [default: <stderr>] logging stream -ll {debug,info,subdefault,default,warn,error} (str) [default: default] logging level -lf {nltm} (str) [default: nltm] logging format` tpo_options - kv (``str``) list of key-value pairs `k::v` separated by `;` in single `'` quotes - custom (``str``) Custom ini file, overrides all other ini files - project (``str``) Project ini file, provides runtime and GCP settings - capture (``str``) Pick capture preset, provides capture baits and targets - settings (``str``) Pick settings preset, configures task settings - genome (``str``) Pick genome preset, configures reference genome","title":"TPO CLI entrypoint"},{"location":"development/adding-capture-panel/","text":"To add a capture panel, use the scripts/add_capture_panel.R script. You will need: * The targets bed file of the panel to add * A (optional) bed file containing regions to exclude (i.e. genotyping positions that you don't want to annotate against) * The reference genome fasta file (you have this if you have done refs_pull ) * The existing annotation regions bed file (you have this if you have done refs_pull ) * A working picard.jar The man page provides instructions on the workflow: $ Rscript scripts/add_capture_panel.R -h Usage: Rscript add_capture_panel.R [options] Generate necessary region files to add a new capture panel to TPO. Returns a MISC.interval_list for cords-misc, a CNV.bed for cords-cnvex and a ANNO.bed for cords-anno (annotation bed + padded [targets - excluded]). All output files are sorted and reduced. Instructions - Run this script - Run 'moke refs_pull' - copy MISC.interval_list -> refs/<build>/capture/ - copy ANNO.bed -> refs/<build>/custom/ - Bump the ROOT_VER in the mokefile and run 'moke refs_push' and 'moke gcp_root_build' - CNV.bed -> tpo/rlibs/cnvex/inst/extdata/capture/ - Run 'moke code_build' (version bumping if needed) Options: --targets=TARGETS Targets bed file --genome=GENOME Genome fasta (for assigning contigs) --exclude=EXCLUDE optional bed file of regions to exclude from annotation (genotyping or backbone SNPs etc.) --annotation=ANNOTATION existing TPO annotated_regions bed file --padding=PADDING padding added to both sides of target bed file for annotation (default:50) --picard=PICARD path to picard.jar -h, --help Show this help message and exit Michigan Center for Translational Pathology (c) 2021","title":"Adding Capture Panel"},{"location":"development/adding-tasks/","text":"Adding new pipelines to TPO involves two steps: * Adding the pipeline scripts * Adding software (binaries, etc.) needed for the pipelines to run Adding pipeline scripts TPO pipelines consist of (at least) two components: * A gcp script ( *_gcp.sh ) which launches the Google Cloud instance and handles input and output of files to the cloud buckets * A docker script ( *_docker.sh ) which is called by a docker container inside of the GCP instance launched above. This script does the \"work\" of whatever bioinformatics pipeline is in use (i.e. a call to samtools will be inside of this script). Updates to the scripts done on your local machine need to be made available to the gcp instances where they are needed. This is accomplished via the code docker image. After changes are made, consider a version bump, then moke images_build -image \"code\" to update the docker locally and moke images_push to copy the code image to GCR. By default the images_build task will tar up the local tpo directory as is, so any changes (even those not staged, committed, etc.) will be made live. Adding new software Software can be made available to TPO scripts in several ways. Deciding which to use is somewhat arbitrary, but should depend on whether the software will be used by multiple pipelines, or just one. The options are * Add to root * This will be available to the gcp instance and any docker images which mount /tpo:/tpo * Add to base docker image * This will be available to base and all docker images that inherit from it * Add to a specific docker image (i.e. cords ) * This will only be available to that image The procedure for adding new files to root involves the following * moke artifacts_pull to download the artifacts to a local directory (specified in mokefile) * wait for pull * copy binary to artifacts/input * update appropriate build script (i.e. base/root/build_tools.sh ) * Add references if needed * moke artifacts_push to rsync changes to the bucket * Bump version ( ROOT_VER ) ( check that no image with a conflicting name/version is present on GCP before proceeding ) * moke gcp_root_build to launch a gcp instance to build the ROOT directory with the new version","title":"Adding Tasks"},{"location":"development/adding-tasks/#adding-pipeline-scripts","text":"TPO pipelines consist of (at least) two components: * A gcp script ( *_gcp.sh ) which launches the Google Cloud instance and handles input and output of files to the cloud buckets * A docker script ( *_docker.sh ) which is called by a docker container inside of the GCP instance launched above. This script does the \"work\" of whatever bioinformatics pipeline is in use (i.e. a call to samtools will be inside of this script). Updates to the scripts done on your local machine need to be made available to the gcp instances where they are needed. This is accomplished via the code docker image. After changes are made, consider a version bump, then moke images_build -image \"code\" to update the docker locally and moke images_push to copy the code image to GCR. By default the images_build task will tar up the local tpo directory as is, so any changes (even those not staged, committed, etc.) will be made live.","title":"Adding pipeline scripts"},{"location":"development/adding-tasks/#adding-new-software","text":"Software can be made available to TPO scripts in several ways. Deciding which to use is somewhat arbitrary, but should depend on whether the software will be used by multiple pipelines, or just one. The options are * Add to root * This will be available to the gcp instance and any docker images which mount /tpo:/tpo * Add to base docker image * This will be available to base and all docker images that inherit from it * Add to a specific docker image (i.e. cords ) * This will only be available to that image The procedure for adding new files to root involves the following * moke artifacts_pull to download the artifacts to a local directory (specified in mokefile) * wait for pull * copy binary to artifacts/input * update appropriate build script (i.e. base/root/build_tools.sh ) * Add references if needed * moke artifacts_push to rsync changes to the bucket * Bump version ( ROOT_VER ) ( check that no image with a conflicting name/version is present on GCP before proceeding ) * moke gcp_root_build to launch a gcp instance to build the ROOT directory with the new version","title":"Adding new software"},{"location":"development/architecture/","text":"Architecture TPO follows as much as possible Design principles: infrastructure is designed and optimized to run on the cloud easy to update and modify take advantage of GCP managed services (block storage, object storage, instances, images, container registries) pipelines should run locally for development and debugging purposes pipelines are implemented as simple shell scripts R packages When adding new dependencies for packages in rlibs . The tpobase image. References Warning : Always do a refs_pull prior to doing a refs_push as files absent locally will be deleted remotely in a refs_push . Images TPO uses two types of images, Docker images and GCP images. The main use for Docker images is to provide a standarized environment for the execution of pipelines, both locally and remotely on GCP. A secondary use case is to provide the TPO code to the using volumes . The docker images are built locally, and distributed using the Google Container Registry (GCR) to the GCP instances. The GCP images are used to launch standardized GCP instances and to provide those instances with large files which cannot be kept in a git repository, such as genomic references. Follwing the execution of a TPO command, the following steps happen: A GCP instance is started using the tpoboot GCP image. This image has all required TPO Docker images pre-loaded. The GCP instance mounts a read-only disk from the tporoot GCP image. This disk provides all required reference files under /tpo . The GCP instance downloads a tpocode Docker image which provides a version of the TPO codebase as a volume. On the GCP instance a Docker container is started from one of the Docker images (e.g. tpocords ), with the tpocode Docker image mounted as a volume under /code . The running Docker container (provided with the volumes and input data) executes the pipeline. Code volume Docker image TPO uses a Docker volume to create snapshots of the TPO codebase (i.e. everything under $TPO_ROOT ) to deploy on the cloud. The images are versioned using the CODE_VER variable in the [TPO] section of the config file. Command Purpose code_build Build the TPO code image locally code_push Push the image to the GCR registry code_pull Pull the image from the GCR registry The resulting image tpocode provides a /code volume which is mounted on the executable Docker images. Executable Docker images TPO uses 4 different Docker images. A base image, called tpobase and 3 derivative images. Image Description tpobase An Ubuntu 18.04 image configured to support the compilation and running of all TPO software tpocords tpobase + custom tools required for the cords pipelines tpocrisp tpobase + custom tools required for the crisp pipelines tpocarat tpobase + custom tools required for the carat pipelines The tpobase image is a complete Ubuntu-based system with commonly required dependencies installed. The derivative images install software specific to certain types of analyses on top of the tpobase image. The following tpo.py commands control the building and push/pull of the Docker images. The images are versioned using the BOOT_VER variable in the [TPO] section of the config file. Command Purpose boot_build Build the TPO docker images locally boot_push Push the images to the GCR registry boot_pull Pull the images from the GCR registry In general, there is typically no need to modify the images outside of TPO development. So the images are built and pushed once using boot_build and boot_push , respectively. GCP images Two GCP images are built, one tpoboot providing a standardized instance for all TPO cloud runs, and another tporoot containing all the files that are necessary for the TPO pipelines to run. The tpoboot and tporoot images are versioned using the BOOT_VER and ROOT_VER variable in the [TPO] section of the config file, respectively. Image Version Purpose tpoboot BOOT_VER Minimal Ubuntu-based image to launch GCP instances tporoot ROOT_VER Large image containing reference and index files The tpoboot image is a minimal Ubuntu-based image with the TPO Docker images ( tpocords , tpocrisp , tpocarat ) pre-loaded. It only needs to be rebuilt if any of the Docker images gets updated, or there is a need to update the Ubuntu OS (e.g. security reasons). The tporoot image is a data volume and cannot be booted. It contains the complete set of assets required to run TPO. This includes genomic reference files, built indexes for various alignment tools, compiled R libraries etc. The contents of this image depend on some settings in the TPO configuration file. For example it contains built alignment indexes only for the genomic references indicated in ALIGN_FASTA . Therefore, the tporoot image has to be in general changed whenever any of the following variables is modified. ALIGN_FASTA ALIGN_NAME ALIGN_MASK QUANT_GTF QUANT_FASTA QUANT_NAME ANNO_VEPCACHE In practice, tporoot is built only once for a given reference genome configuration. TPO provides config template files for GRCh38 following community recommentations in $TPO_ROOT/config . Building the tporoot image requires considerable computational resources and cannot be done on a laptop or desktop computer. Currently the only way to create a new image is on the GCP. This process takes approximately 4 hours. Command Purpose gcp_boot_build Build the tpoboot GCP image gcp_root_build Build the tporoot GCP image gcp_root_push Create a tar archive of the tporoot image and save it in a GCP bucket gcp_root_pull Download and extract the tar archive locally If intending to run TPO locally, following gcp_root_build , the user needs to execute gcp_root_push and gcp_root_pull to transfer the contents of tporoot into a local directory, as specified by the ROOT variable in the [RUNTIME] section of the config file. GCP instance A GCP instance based on the tpoboot-$BOOT_VER image can be created using the following command: $TPO_ROOT/tpo.py -config config/my_config.ini gcp_instance Warning! Please not this command will start an instance which will be running indefinitely, possibly incurring high costs. The instance should be suspended, stopped, or terminated when not in use. Please see GCP documentation on how to accomplish this from either the CLI or cloud console. Add user to docker group in order to use docker: sudo usermod -aG docker <userID> Test cases ./tpo.py -config mokefile_grch38-tpo-latest-v2-e106.ini cords_align HLGWCDRXX.1 SI_30806 \\ gs://tpo-refs/v2/test/fastq/tiny_SI_30806_HLGWCDRXX_0_1.fq.gz \\ gs://tpo-refs/v2/test/fastq/tiny_SI_30806_HLGWCDRXX_0_2.fq.gz Developer Notes after Java update, check if bbmap JNI still compiles (i.e. if JAVA_HOME is valid) lots of error messages can be lost because we are running docker in -d, disable if debugging All hard-coded variables in build_refs need to go in order to support alternate species updating Ensembl: downlad fasta, bgzip, faidx, cdna for kallisto, gtf fix tags","title":"Architecture"},{"location":"development/architecture/#architecture","text":"TPO follows as much as possible Design principles: infrastructure is designed and optimized to run on the cloud easy to update and modify take advantage of GCP managed services (block storage, object storage, instances, images, container registries) pipelines should run locally for development and debugging purposes pipelines are implemented as simple shell scripts","title":"Architecture"},{"location":"development/architecture/#r-packages","text":"When adding new dependencies for packages in rlibs . The tpobase image.","title":"R packages"},{"location":"development/architecture/#references","text":"Warning : Always do a refs_pull prior to doing a refs_push as files absent locally will be deleted remotely in a refs_push .","title":"References"},{"location":"development/architecture/#images","text":"TPO uses two types of images, Docker images and GCP images. The main use for Docker images is to provide a standarized environment for the execution of pipelines, both locally and remotely on GCP. A secondary use case is to provide the TPO code to the using volumes . The docker images are built locally, and distributed using the Google Container Registry (GCR) to the GCP instances. The GCP images are used to launch standardized GCP instances and to provide those instances with large files which cannot be kept in a git repository, such as genomic references. Follwing the execution of a TPO command, the following steps happen: A GCP instance is started using the tpoboot GCP image. This image has all required TPO Docker images pre-loaded. The GCP instance mounts a read-only disk from the tporoot GCP image. This disk provides all required reference files under /tpo . The GCP instance downloads a tpocode Docker image which provides a version of the TPO codebase as a volume. On the GCP instance a Docker container is started from one of the Docker images (e.g. tpocords ), with the tpocode Docker image mounted as a volume under /code . The running Docker container (provided with the volumes and input data) executes the pipeline.","title":"Images"},{"location":"development/architecture/#code-volume-docker-image","text":"TPO uses a Docker volume to create snapshots of the TPO codebase (i.e. everything under $TPO_ROOT ) to deploy on the cloud. The images are versioned using the CODE_VER variable in the [TPO] section of the config file. Command Purpose code_build Build the TPO code image locally code_push Push the image to the GCR registry code_pull Pull the image from the GCR registry The resulting image tpocode provides a /code volume which is mounted on the executable Docker images.","title":"Code volume Docker image"},{"location":"development/architecture/#executable-docker-images","text":"TPO uses 4 different Docker images. A base image, called tpobase and 3 derivative images. Image Description tpobase An Ubuntu 18.04 image configured to support the compilation and running of all TPO software tpocords tpobase + custom tools required for the cords pipelines tpocrisp tpobase + custom tools required for the crisp pipelines tpocarat tpobase + custom tools required for the carat pipelines The tpobase image is a complete Ubuntu-based system with commonly required dependencies installed. The derivative images install software specific to certain types of analyses on top of the tpobase image. The following tpo.py commands control the building and push/pull of the Docker images. The images are versioned using the BOOT_VER variable in the [TPO] section of the config file. Command Purpose boot_build Build the TPO docker images locally boot_push Push the images to the GCR registry boot_pull Pull the images from the GCR registry In general, there is typically no need to modify the images outside of TPO development. So the images are built and pushed once using boot_build and boot_push , respectively.","title":"Executable Docker images"},{"location":"development/architecture/#gcp-images","text":"Two GCP images are built, one tpoboot providing a standardized instance for all TPO cloud runs, and another tporoot containing all the files that are necessary for the TPO pipelines to run. The tpoboot and tporoot images are versioned using the BOOT_VER and ROOT_VER variable in the [TPO] section of the config file, respectively. Image Version Purpose tpoboot BOOT_VER Minimal Ubuntu-based image to launch GCP instances tporoot ROOT_VER Large image containing reference and index files The tpoboot image is a minimal Ubuntu-based image with the TPO Docker images ( tpocords , tpocrisp , tpocarat ) pre-loaded. It only needs to be rebuilt if any of the Docker images gets updated, or there is a need to update the Ubuntu OS (e.g. security reasons). The tporoot image is a data volume and cannot be booted. It contains the complete set of assets required to run TPO. This includes genomic reference files, built indexes for various alignment tools, compiled R libraries etc. The contents of this image depend on some settings in the TPO configuration file. For example it contains built alignment indexes only for the genomic references indicated in ALIGN_FASTA . Therefore, the tporoot image has to be in general changed whenever any of the following variables is modified. ALIGN_FASTA ALIGN_NAME ALIGN_MASK QUANT_GTF QUANT_FASTA QUANT_NAME ANNO_VEPCACHE In practice, tporoot is built only once for a given reference genome configuration. TPO provides config template files for GRCh38 following community recommentations in $TPO_ROOT/config . Building the tporoot image requires considerable computational resources and cannot be done on a laptop or desktop computer. Currently the only way to create a new image is on the GCP. This process takes approximately 4 hours. Command Purpose gcp_boot_build Build the tpoboot GCP image gcp_root_build Build the tporoot GCP image gcp_root_push Create a tar archive of the tporoot image and save it in a GCP bucket gcp_root_pull Download and extract the tar archive locally If intending to run TPO locally, following gcp_root_build , the user needs to execute gcp_root_push and gcp_root_pull to transfer the contents of tporoot into a local directory, as specified by the ROOT variable in the [RUNTIME] section of the config file.","title":"GCP images"},{"location":"development/architecture/#gcp-instance","text":"A GCP instance based on the tpoboot-$BOOT_VER image can be created using the following command: $TPO_ROOT/tpo.py -config config/my_config.ini gcp_instance Warning! Please not this command will start an instance which will be running indefinitely, possibly incurring high costs. The instance should be suspended, stopped, or terminated when not in use. Please see GCP documentation on how to accomplish this from either the CLI or cloud console. Add user to docker group in order to use docker: sudo usermod -aG docker <userID>","title":"GCP instance"},{"location":"development/architecture/#test-cases","text":"./tpo.py -config mokefile_grch38-tpo-latest-v2-e106.ini cords_align HLGWCDRXX.1 SI_30806 \\ gs://tpo-refs/v2/test/fastq/tiny_SI_30806_HLGWCDRXX_0_1.fq.gz \\ gs://tpo-refs/v2/test/fastq/tiny_SI_30806_HLGWCDRXX_0_2.fq.gz","title":"Test cases"},{"location":"development/architecture/#developer-notes","text":"after Java update, check if bbmap JNI still compiles (i.e. if JAVA_HOME is valid) lots of error messages can be lost because we are running docker in -d, disable if debugging All hard-coded variables in build_refs need to go in order to support alternate species updating Ensembl: downlad fasta, bgzip, faidx, cdna for kallisto, gtf fix tags","title":"Developer Notes"},{"location":"development/debugging/","text":"Debugging updates/build Debugging gcp_boot_build During gcp_boot_build startup script is terminated without clear error message. At least once this occurred because apt-get upgrade installed a new version of systemd , which is then restarted. Since systemd runs the startup script it gets killed during the restart. Debugging a failed instance How to do it/where to look Suppose you have an instance that never stopped running, or failed. The first place you should look is within that instance on google cloud.To do this: 1. Navigate to the VM instances tab on google cloud and select the instance. 2. Click the SSH button and connect to the instance. A terminal-like window will pop up. 3. The first thing to check is config.txt cat /work/job/config.txt . Verify that your parameters are what you expect them to be. 4. Next take a look at the docker script log cat /work/job/docker_script.log . This can show you if/where your processes were interrupted. 5. Next check out the gcp script log cat /work/job/gcp_script.log . This will show you the process your instance has progressed through. 6. Another possibility is that you have exceeded the instance's provided memory. To check this, run htop . THis will show you the memory and swap usage. You can increase the memory by changing the default machine used. The default is most likely machine=\"n2-highcpu-32\" . Instead you can try machine=\"n2-standard-8\" or if you're feeling spendy machine=\"n2-highcpu-64\" 7. You can also check disk usage using du -h . If you are at/near 100% usage, you should consider changing the disk size. The default is most likely work_disk=\"250G\" instead try work_disk=\"500G\"","title":"Debugging"},{"location":"development/debugging/#debugging-updatesbuild","text":"","title":"Debugging updates/build"},{"location":"development/debugging/#debugging-gcp_boot_build","text":"During gcp_boot_build startup script is terminated without clear error message. At least once this occurred because apt-get upgrade installed a new version of systemd , which is then restarted. Since systemd runs the startup script it gets killed during the restart.","title":"Debugging gcp_boot_build"},{"location":"development/debugging/#debugging-a-failed-instance","text":"","title":"Debugging a failed instance"},{"location":"development/debugging/#how-to-do-itwhere-to-look","text":"Suppose you have an instance that never stopped running, or failed. The first place you should look is within that instance on google cloud.To do this: 1. Navigate to the VM instances tab on google cloud and select the instance. 2. Click the SSH button and connect to the instance. A terminal-like window will pop up. 3. The first thing to check is config.txt cat /work/job/config.txt . Verify that your parameters are what you expect them to be. 4. Next take a look at the docker script log cat /work/job/docker_script.log . This can show you if/where your processes were interrupted. 5. Next check out the gcp script log cat /work/job/gcp_script.log . This will show you the process your instance has progressed through. 6. Another possibility is that you have exceeded the instance's provided memory. To check this, run htop . THis will show you the memory and swap usage. You can increase the memory by changing the default machine used. The default is most likely machine=\"n2-highcpu-32\" . Instead you can try machine=\"n2-standard-8\" or if you're feeling spendy machine=\"n2-highcpu-64\" 7. You can also check disk usage using du -h . If you are at/near 100% usage, you should consider changing the disk size. The default is most likely work_disk=\"250G\" instead try work_disk=\"500G\"","title":"How to do it/where to look"},{"location":"development/repository/","text":"Repository Top-Level Directories $TPO_ROOT \u251c\u2500\u2500 base \u251c\u2500\u2500 config \u251c\u2500\u2500 examples \u251c\u2500\u2500 cli \u251c\u2500\u2500 pipe \u251c\u2500\u2500 rlibs \u251c\u2500\u2500 scripts \u251c\u2500\u2500 task \u251c\u2500\u2500 test cli Implementation (python modules) of the main TPO CLI interface for all TPO tasks. Each TPO task is implemented as a python function for which a commandline interface is created using the moke python package. Tasks are grouped by type of analysis (e.g. DNA analyses are in cli/cords.py ) and housekeeping / development tasks cli/devel.py . Internal functions setting up the CLI (e.g. defaults) are in cli/__init__.py . task Implementation (Shell scripts) of the actual TPO tasks. Each directory in task implements a single analysis task. For example, task/cords-align implements the the DNA alignment pipeline. Every task is comprised of at least three shell scripts. The actual analysis code *_docker.sh the local launcher script *_local.sh and the remote (GCP) launcher *_gcp.sh . pipe Implementation (Shell scripts) of TPO pipelines (wrapper of TPO tasks). rlibs R-libraries implementing fundamental TPO functionality, used by TPO tasks. base Contains shell scripts and Dockerfiles required to bootstrap the TPO infrastructure of docker and GCP images. config Configuration (ini) file templates and documentation. examples Short scripts files covering TPO use-cases. scripts Misc. scripts. test TPO test scripts.","title":"Repository"},{"location":"development/repository/#repository","text":"","title":"Repository"},{"location":"development/repository/#top-level-directories","text":"$TPO_ROOT \u251c\u2500\u2500 base \u251c\u2500\u2500 config \u251c\u2500\u2500 examples \u251c\u2500\u2500 cli \u251c\u2500\u2500 pipe \u251c\u2500\u2500 rlibs \u251c\u2500\u2500 scripts \u251c\u2500\u2500 task \u251c\u2500\u2500 test","title":"Top-Level Directories"},{"location":"development/repository/#cli","text":"Implementation (python modules) of the main TPO CLI interface for all TPO tasks. Each TPO task is implemented as a python function for which a commandline interface is created using the moke python package. Tasks are grouped by type of analysis (e.g. DNA analyses are in cli/cords.py ) and housekeeping / development tasks cli/devel.py . Internal functions setting up the CLI (e.g. defaults) are in cli/__init__.py .","title":"cli"},{"location":"development/repository/#task","text":"Implementation (Shell scripts) of the actual TPO tasks. Each directory in task implements a single analysis task. For example, task/cords-align implements the the DNA alignment pipeline. Every task is comprised of at least three shell scripts. The actual analysis code *_docker.sh the local launcher script *_local.sh and the remote (GCP) launcher *_gcp.sh .","title":"task"},{"location":"development/repository/#pipe","text":"Implementation (Shell scripts) of TPO pipelines (wrapper of TPO tasks).","title":"pipe"},{"location":"development/repository/#rlibs","text":"R-libraries implementing fundamental TPO functionality, used by TPO tasks.","title":"rlibs"},{"location":"development/repository/#base","text":"Contains shell scripts and Dockerfiles required to bootstrap the TPO infrastructure of docker and GCP images.","title":"base"},{"location":"development/repository/#config","text":"Configuration (ini) file templates and documentation.","title":"config"},{"location":"development/repository/#examples","text":"Short scripts files covering TPO use-cases.","title":"examples"},{"location":"development/repository/#scripts","text":"Misc. scripts.","title":"scripts"},{"location":"development/repository/#test","text":"TPO test scripts.","title":"test"},{"location":"development/unit-testing/","text":"Unit Testing Testing can be accomplished using the built in unit_test task: $ ./tpo.py unit_test -h usage: tpo.py unit_test [-h] [-test TEST] [-start_from START_FROM] [--skip_r_tests] [--skip_cleanup] [--dry_run] options: -h, --help show this help message and exit -test TEST (str) [default: HCC2218] what unit test sample to test -start_from START_FROM (str) [default: align] which pipeline to start from --skip_r_tests (``bool``) Skip the R test --skip_cleanup (``bool``) Skip removing alignments --dry_run (``bool``) save rather than run commands Getting started First set up the necessary fields in the mokefile TEST section to specify where on GCP and where locally the results should end up, for example: [TEST] UNIT_TEST_BUCKET = mctp-hopkinal/unit_test UNIT_TEST = /home/hopkinal/tpounit/ Running Automated Tests The first time you run unit_test with a given UNIT_TEST_BUCKET it will run the complete test from start to finish (ignoring -start_from ), saving the results to <UNIT_TEST_BUCKET>/base . ./tpo.py -config mokefile.ini unit_test Subsequent tests will generate a unique run ID, and can start anywhere in the process, taking results from $UNIT_TEST_BUCKET/base/ as their starting point. Run a test from the annotation stage: ./tpo.py -config mokefile.ini unit_test -start_from anno What it does By default, unit_test will * Run necessary TPO piplines * Download the final results (vault output) to the CWD * Run the specified test in ./rlib/carat/inst/extdata/test/ * Save output to <runid>_tpo_unit_test.results To run the pipelines (and skip syncing results and running R tests), run with --skip_r_test . To generate the testing script but not run anything, use --dry_run . Output A typical output (using -start_from vault ) looks like $ ./tpo.py -config mokefile-unit_test.ini unit_test -start_from vault moke DEFAULT 2023-05-10 16:44:49,336 task: unit_test moke DEFAULT 2023-05-10 16:44:49,336 dry_run: False moke DEFAULT 2023-05-10 16:44:49,336 mode: tnr moke DEFAULT 2023-05-10 16:44:49,336 skip_r_tests: False moke DEFAULT 2023-05-10 16:44:49,336 start_from: vault moke DEFAULT 2023-05-10 16:52:09,073 shell[0]: /mctp/users/hopkinal/deployment/bleeding/tpo/pipe/unit-test/unit_test.sh moke DEFAULT 2023-05-10 16:52:09,073 stdout: runid: 5ee22c96 $ cat 5ee22c96_tpo_unit_test.results Wed May 10 16:44:51 EDT 2023 TPO_CODE_VER: 2.6-14-hopkinal Wed May 10 16:44:51 EDT 2023 TPO_ROOT_VER: 2.6-3-grch38-tpo-108 Wed May 10 16:44:51 EDT 2023 TPO_BOOT_VER: 2.6-11 Wed May 10 16:44:51 EDT 2023 TPO_REFS_VER: tpo-refs/v2 Wed May 10 16:44:51 EDT 2023 Running TPO unit test on Tumor-Normal-RNA data from vault Wed May 10 16:44:51 EDT 2023 Making Vault Wed May 10 16:51:45 EDT 2023 Running R Unit Tests == Testing test-vault.R ======================================================== [ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 5 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 6 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 7 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 8 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 9 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 10 ] Done! Wed May 10 16:52:09 EDT 2023 Finished Adding unit tests The unit_test TPO task uses the R testthat package. Add new tests to ./rlibs/carat/inst/extdata/test/generic.R . If you are developing functions that operate on any TPO *vault objects, you can add tests to this script and then re-run them (on the vault in <UNIT_TEST_BUCKET>/base ) by running ./tpo.py -config mokefile.ini unit_test -start_from R Adding unit test samples If you want to add samples other than HCC2218 to test on, for example HCC1395 , you will need to do a few things * First, make a new config file for the test in rlibs/carat/inst/extdata/test/HCC1395.config (see HCC2218 example) * Next, place the fastqs into gs://tpo-refs/v2/test/fastq/ , with file names like <UNIT_TEST_LIB_T>_unittest_1.fq.gz * Run the new test with ./tpo.py -config mokefile.ini unit_test -start_from align -test HCC1395 My Test Worked! What does that mean? If unit_test ran with no errors and all R test passed, this means: * Your mokefile specifies a working combination of CODE , ROOT , BOOT , and REFS * Any changes you made to CODE did not seriously break the pipelines * The version of carat installed locally is compatible with the vaults made by the tested version of the pipeline It does not mean: * There are no bugs in the new code * The variant filters are working properly I don't want to deal with this moke task stuff, can I still use these tests? The tests in ./rlibs/carat/test/generic.R are reasonably general, and most of them will pass on any vault, so you can in theory run v <- 'path/to/my/favorite/vault.rds' testthat::test_file('./rlibs/carat/test/generic.R')","title":"Unit Testing"},{"location":"development/unit-testing/#unit-testing","text":"Testing can be accomplished using the built in unit_test task: $ ./tpo.py unit_test -h usage: tpo.py unit_test [-h] [-test TEST] [-start_from START_FROM] [--skip_r_tests] [--skip_cleanup] [--dry_run] options: -h, --help show this help message and exit -test TEST (str) [default: HCC2218] what unit test sample to test -start_from START_FROM (str) [default: align] which pipeline to start from --skip_r_tests (``bool``) Skip the R test --skip_cleanup (``bool``) Skip removing alignments --dry_run (``bool``) save rather than run commands","title":"Unit Testing"},{"location":"development/unit-testing/#getting-started","text":"First set up the necessary fields in the mokefile TEST section to specify where on GCP and where locally the results should end up, for example: [TEST] UNIT_TEST_BUCKET = mctp-hopkinal/unit_test UNIT_TEST = /home/hopkinal/tpounit/","title":"Getting started"},{"location":"development/unit-testing/#running-automated-tests","text":"The first time you run unit_test with a given UNIT_TEST_BUCKET it will run the complete test from start to finish (ignoring -start_from ), saving the results to <UNIT_TEST_BUCKET>/base . ./tpo.py -config mokefile.ini unit_test Subsequent tests will generate a unique run ID, and can start anywhere in the process, taking results from $UNIT_TEST_BUCKET/base/ as their starting point. Run a test from the annotation stage: ./tpo.py -config mokefile.ini unit_test -start_from anno","title":"Running Automated Tests"},{"location":"development/unit-testing/#what-it-does","text":"By default, unit_test will * Run necessary TPO piplines * Download the final results (vault output) to the CWD * Run the specified test in ./rlib/carat/inst/extdata/test/ * Save output to <runid>_tpo_unit_test.results To run the pipelines (and skip syncing results and running R tests), run with --skip_r_test . To generate the testing script but not run anything, use --dry_run .","title":"What it does"},{"location":"development/unit-testing/#output","text":"A typical output (using -start_from vault ) looks like $ ./tpo.py -config mokefile-unit_test.ini unit_test -start_from vault moke DEFAULT 2023-05-10 16:44:49,336 task: unit_test moke DEFAULT 2023-05-10 16:44:49,336 dry_run: False moke DEFAULT 2023-05-10 16:44:49,336 mode: tnr moke DEFAULT 2023-05-10 16:44:49,336 skip_r_tests: False moke DEFAULT 2023-05-10 16:44:49,336 start_from: vault moke DEFAULT 2023-05-10 16:52:09,073 shell[0]: /mctp/users/hopkinal/deployment/bleeding/tpo/pipe/unit-test/unit_test.sh moke DEFAULT 2023-05-10 16:52:09,073 stdout: runid: 5ee22c96 $ cat 5ee22c96_tpo_unit_test.results Wed May 10 16:44:51 EDT 2023 TPO_CODE_VER: 2.6-14-hopkinal Wed May 10 16:44:51 EDT 2023 TPO_ROOT_VER: 2.6-3-grch38-tpo-108 Wed May 10 16:44:51 EDT 2023 TPO_BOOT_VER: 2.6-11 Wed May 10 16:44:51 EDT 2023 TPO_REFS_VER: tpo-refs/v2 Wed May 10 16:44:51 EDT 2023 Running TPO unit test on Tumor-Normal-RNA data from vault Wed May 10 16:44:51 EDT 2023 Making Vault Wed May 10 16:51:45 EDT 2023 Running R Unit Tests == Testing test-vault.R ======================================================== [ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 5 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 6 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 7 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 8 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 9 ] [ FAIL 0 | WARN 0 | SKIP 0 | PASS 10 ] Done! Wed May 10 16:52:09 EDT 2023 Finished","title":"Output"},{"location":"development/unit-testing/#adding-unit-tests","text":"The unit_test TPO task uses the R testthat package. Add new tests to ./rlibs/carat/inst/extdata/test/generic.R . If you are developing functions that operate on any TPO *vault objects, you can add tests to this script and then re-run them (on the vault in <UNIT_TEST_BUCKET>/base ) by running ./tpo.py -config mokefile.ini unit_test -start_from R","title":"Adding unit tests"},{"location":"development/unit-testing/#adding-unit-test-samples","text":"If you want to add samples other than HCC2218 to test on, for example HCC1395 , you will need to do a few things * First, make a new config file for the test in rlibs/carat/inst/extdata/test/HCC1395.config (see HCC2218 example) * Next, place the fastqs into gs://tpo-refs/v2/test/fastq/ , with file names like <UNIT_TEST_LIB_T>_unittest_1.fq.gz * Run the new test with ./tpo.py -config mokefile.ini unit_test -start_from align -test HCC1395","title":"Adding unit test samples"},{"location":"development/unit-testing/#my-test-worked-what-does-that-mean","text":"If unit_test ran with no errors and all R test passed, this means: * Your mokefile specifies a working combination of CODE , ROOT , BOOT , and REFS * Any changes you made to CODE did not seriously break the pipelines * The version of carat installed locally is compatible with the vaults made by the tested version of the pipeline It does not mean: * There are no bugs in the new code * The variant filters are working properly","title":"My Test Worked! What does that mean?"},{"location":"development/unit-testing/#i-dont-want-to-deal-with-this-moke-task-stuff-can-i-still-use-these-tests","text":"The tests in ./rlibs/carat/test/generic.R are reasonably general, and most of them will pass on any vault, so you can in theory run v <- 'path/to/my/favorite/vault.rds' testthat::test_file('./rlibs/carat/test/generic.R')","title":"I don't want to deal with this moke task stuff, can I still use these tests?"},{"location":"development/updating-ensembl/","text":"Updating TPO to a newer Ensembl version Updating TPO to a newer Ensembl version is relatively straightforward, but requires multiple steps as typically both software (i.e. VEP ) and reference files need to be updated in lock-step. Both Docker and GCP images will need to be rebuild to incorporate the changes. The process consists of the following steps which will result in an update of VEP , the VEP annotation sources which include Ensembl i.e. vep_cache , and Ensembl GTF reference files . Updating VEP to support the new annotation sources and rebuilding (at least) the tpocarat Docker image. Building a new GCP instance image (i.e. tpoboot ) containing the updated Docker images. Downloading and modifying GTF and FASTA files from Ensembl, downloading VEP cache files. Creating a new tporefs reference image containing the updated GTF , FASTA , and VEP cache files. 1. Update VEP and build new Docker images We need to update VEP to a version which will support the latest Ensembl VEP annotation sources (i.e. vep_cache ). Identify the required VEP version. Find latest VEP on GitHub , it is good to wait for a first bug-fix release e.g. 106.1 . Check if latest VEP version is also available on Bioconda , if not it is better to wait. In TPO code edit base/images/tpocarat/Dockerfile to reflect the desired VEP version from Bioconda. Create updated Docker images with new VEP. For consistency it is reasonable to update all images not only tpocarat . In the TPO config file bump BOOT_VER to not override the old images. If necessary do a refs_pull on the instance, as some files (e.g. installers) are required to build Docker images. Do a full boot_build , ideally without reusing the Docker cache. We now have a set of new Docker images locally. 2. Build new GCP instance image Push updated images to the GCR using boot_push Create updated GCP instance image using gcp_boot_build We now have a set GCP instance image with the updated Docker images. 3. Create Updated reference files Download and Ungzip the following files, replacing 106 with the desired version http://ftp.ensembl.org/pub/release-106/variation/vep/homo_sapiens_merged_vep_106_GRCh38.tar.gz http://ftp.ensembl.org/pub/release-106/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz http://ftp.ensembl.org/pub/release-106/gtf/homo_sapiens/Homo_sapiens.GRCh38.106.gtf.gz Fix GTF file by combining multiple tag features into one tags feature cat Homo_sapiens.GRCh38.108.gtf | scripts/make_gtf_tags.py fix_tags > Homo_sapiens.GRCh38.108-tags.gtf Create clean GTF and FASTA file, In an interactive R-session, walk through the steps in scripts/make_ensembl_rna_gtf.R . This will create a cleaned up GTF file, and a corresponding FASTA file. Test if a new CODAC configuration file can be created from the new GTF file. makeAnnotations(\"Homo_sapiens.GRCh38.108.clean.gtf\", \"hg38\") If it fails or gives warnings it means the structure of the GTF file changed, do not ignore investigate. 4. Create a new tporoot We will use the new tpoboot instance image to build a new tporoot with the updated reference files Copy all new and updated files in the right places. ``` original GTF file cp Homo_sapiens.GRCh38.108.gtf $XXX/v2/refs/refs/grch38/ensembl/grch38.108.all.gtf cp Homo_sapiens.GRCh38.cdna.all.fa $XXX/v2/refs/refs/grch38/ensembl/grch38.108.all.cdna.fa clean GTF and FASTA file cp Homo_sapiens.GRCh38.108.clean.gtf $XXX/v2/refs/refs/grch38/ensembl/grch38.108.clean.gtf cp Homo_sapiens.GRCh38.108.cdna.clean.fa $XXX/v2/refs/refs/grch38/ensembl/grch38.108.all.cdna.fa VEP file cp homo_sapiens_merged_vep_108_GRCh38.tar.gz $XXX/v2/refs/refs/grch38/ensembl/grch38.108.merged.vep.tar ` 2. Push updated reference files to the cloud using refs_push 3. Build updated tporoot image. - Update ROOT_VER to newer version reflecting update of both BOOT_VER and bump in Ensemble version, e.g. ``` ROOT_VER = 2.5-0-grch38-tpo-108 ``` - Trigger build of new tporoot GCP image using gcp_root_build`","title":"Updating Ensembl"},{"location":"development/updating-ensembl/#updating-tpo-to-a-newer-ensembl-version","text":"Updating TPO to a newer Ensembl version is relatively straightforward, but requires multiple steps as typically both software (i.e. VEP ) and reference files need to be updated in lock-step. Both Docker and GCP images will need to be rebuild to incorporate the changes. The process consists of the following steps which will result in an update of VEP , the VEP annotation sources which include Ensembl i.e. vep_cache , and Ensembl GTF reference files . Updating VEP to support the new annotation sources and rebuilding (at least) the tpocarat Docker image. Building a new GCP instance image (i.e. tpoboot ) containing the updated Docker images. Downloading and modifying GTF and FASTA files from Ensembl, downloading VEP cache files. Creating a new tporefs reference image containing the updated GTF , FASTA , and VEP cache files.","title":"Updating TPO to a newer Ensembl version"},{"location":"development/updating-ensembl/#1-update-vep-and-build-new-docker-images","text":"We need to update VEP to a version which will support the latest Ensembl VEP annotation sources (i.e. vep_cache ). Identify the required VEP version. Find latest VEP on GitHub , it is good to wait for a first bug-fix release e.g. 106.1 . Check if latest VEP version is also available on Bioconda , if not it is better to wait. In TPO code edit base/images/tpocarat/Dockerfile to reflect the desired VEP version from Bioconda. Create updated Docker images with new VEP. For consistency it is reasonable to update all images not only tpocarat . In the TPO config file bump BOOT_VER to not override the old images. If necessary do a refs_pull on the instance, as some files (e.g. installers) are required to build Docker images. Do a full boot_build , ideally without reusing the Docker cache. We now have a set of new Docker images locally.","title":"1. Update VEP and build new Docker images"},{"location":"development/updating-ensembl/#2-build-new-gcp-instance-image","text":"Push updated images to the GCR using boot_push Create updated GCP instance image using gcp_boot_build We now have a set GCP instance image with the updated Docker images.","title":"2. Build new GCP instance image"},{"location":"development/updating-ensembl/#3-create-updated-reference-files","text":"Download and Ungzip the following files, replacing 106 with the desired version http://ftp.ensembl.org/pub/release-106/variation/vep/homo_sapiens_merged_vep_106_GRCh38.tar.gz http://ftp.ensembl.org/pub/release-106/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz http://ftp.ensembl.org/pub/release-106/gtf/homo_sapiens/Homo_sapiens.GRCh38.106.gtf.gz Fix GTF file by combining multiple tag features into one tags feature cat Homo_sapiens.GRCh38.108.gtf | scripts/make_gtf_tags.py fix_tags > Homo_sapiens.GRCh38.108-tags.gtf Create clean GTF and FASTA file, In an interactive R-session, walk through the steps in scripts/make_ensembl_rna_gtf.R . This will create a cleaned up GTF file, and a corresponding FASTA file. Test if a new CODAC configuration file can be created from the new GTF file. makeAnnotations(\"Homo_sapiens.GRCh38.108.clean.gtf\", \"hg38\") If it fails or gives warnings it means the structure of the GTF file changed, do not ignore investigate.","title":"3. Create Updated reference files"},{"location":"development/updating-ensembl/#4-create-a-new-tporoot","text":"We will use the new tpoboot instance image to build a new tporoot with the updated reference files Copy all new and updated files in the right places. ```","title":"4. Create a new tporoot"},{"location":"development/updating-ensembl/#original-gtf-file","text":"cp Homo_sapiens.GRCh38.108.gtf $XXX/v2/refs/refs/grch38/ensembl/grch38.108.all.gtf cp Homo_sapiens.GRCh38.cdna.all.fa $XXX/v2/refs/refs/grch38/ensembl/grch38.108.all.cdna.fa","title":"original GTF file"},{"location":"development/updating-ensembl/#clean-gtf-and-fasta-file","text":"cp Homo_sapiens.GRCh38.108.clean.gtf $XXX/v2/refs/refs/grch38/ensembl/grch38.108.clean.gtf cp Homo_sapiens.GRCh38.108.cdna.clean.fa $XXX/v2/refs/refs/grch38/ensembl/grch38.108.all.cdna.fa","title":"clean GTF and FASTA file"},{"location":"development/updating-ensembl/#vep-file","text":"cp homo_sapiens_merged_vep_108_GRCh38.tar.gz $XXX/v2/refs/refs/grch38/ensembl/grch38.108.merged.vep.tar ` 2. Push updated reference files to the cloud using refs_push 3. Build updated tporoot image. - Update ROOT_VER to newer version reflecting update of both BOOT_VER and bump in Ensemble version, e.g. ``` ROOT_VER = 2.5-0-grch38-tpo-108 ``` - Trigger build of new tporoot GCP image using gcp_root_build`","title":"VEP file"},{"location":"development/updating-panel-of-normals/","text":"Create a multi-sample cohort BCF file <COHORT_BCF> out of multiple unfiltered VCFs from cords-somatic . The purpose of the panel is to quantify noisy positions as detected by TNScope therefore the input VCF files should be -somatic-tnscope.vcf.gz . Using multiple threads <T> will significantly speed-up this process. bcftools merge --threads <T> -Ob -o <COHORT_BCF> -m both <TNSCOPE SOMATIC VCFs> bcftools index --threads <T> <COHORT_BCF> The -m both tells bcftools to create separate multi-allelic records for indels and SNPs, which will be useful later as their error profiles are very different. No filtering should be applied to this file. Cross-tabulate tumor-normal alt-allele allele-depth AD counts. This creates a new INFO/ADX field. Which represents and n+1 * n+1 matrix tabulating, where n is the maximal tabulated AD in the tumor and normal samples. The script is relatively slow, but can be executed in parallel. parallel -j <THREADS> ad-crosstab.py -chrom {} <COHORT_BCF> ADX_{}.vcf ::: chr{1..22} chrX chrY Compress using bgzip and index using bcftools parallel -j <THREADS> bgzip {} ::: PON_*vcf parallel -j <THREADS> bcftools index {} ::: PON_*vcf.gz Concatenate VCF files into a single PON bcftools concat --threads <THREADS> -o PON.bcf -Ob PON_chr{{1..22},X,Y}.vcf.gz bcftools index--threads <THREADS> PON.bcf","title":"Updating Panel-of-Normals"},{"location":"development/updating-references/","text":"Updating References refs_pull Sources of references to consider Notes GnomAD v4.0 https://gnomad.broadinstitute.org/news/2023-11-gnomad-v4-0/ https://gnomad.broadinstitute.org/news/2023-11-genetic-ancestry/ Broad Genome References https://console.cloud.google.com/marketplace/product/broad-institute/references Gnomad https://console.cloud.google.com/marketplace/product/broad-institute/gnomad Human Variant https://console.cloud.google.com/marketplace/product/bigquery-public-data/human-variant-annotation-public This seems to be the 3.1.2 callset: https://www.biorxiv.org/content/10.1101/2023.01.23.525248v3 https://gnomad.broadinstitute.org/downloads#v3-hgdp-1kg","title":"Updating References"},{"location":"development/updating-references/#updating-references","text":"refs_pull","title":"Updating References"},{"location":"development/updating-references/#sources-of-references-to-consider","text":"","title":"Sources of references to consider"},{"location":"development/updating-references/#notes","text":"","title":"Notes"},{"location":"development/updating-references/#gnomad-v40","text":"https://gnomad.broadinstitute.org/news/2023-11-gnomad-v4-0/ https://gnomad.broadinstitute.org/news/2023-11-genetic-ancestry/ Broad Genome References https://console.cloud.google.com/marketplace/product/broad-institute/references Gnomad https://console.cloud.google.com/marketplace/product/broad-institute/gnomad Human Variant https://console.cloud.google.com/marketplace/product/bigquery-public-data/human-variant-annotation-public This seems to be the 3.1.2 callset: https://www.biorxiv.org/content/10.1101/2023.01.23.525248v3 https://gnomad.broadinstitute.org/downloads#v3-hgdp-1kg","title":"GnomAD v4.0"},{"location":"development/use-tpo-in-local-mode/","text":"Run Cords analysis locally step1. Configurate Runtime Please follow the instructions on page Installation . step2. Modify runtime setting in config file Please follow the instructions on page runtime-local-mode . Step3. Add --local in running command. Examples: align ./tpo.py -config mokefile_grch38-tpo-latest-v2-e106.ini cords_align SE_tumor SI_31729 gs://mctp-tpo/jhgong-tpo-tiny-true/tiny_mctp_SI_31729_H7MFFDSXY_2_1.fq.gz gs://mctp-tpo/jhgong-tpo-tiny-true/tiny_mctp_SI_31729_H7MFFDSXY_2_2.fq.gz --local post ./tpo.py -config mokefile_grch38-tpo-latest-v2-e106.ini cords_postalign SE_tumor gs://mctp-tpo/jhgong-tpo-tiny-true/repo/cords-align/SE_tumor --local somatic ./tpo.py -config mokefile_grch38-tpo-latest-v2-e106.ini cords_somatic SE_to_tumor -alnt gs://mctp-tpo/jhgong-tpo-tiny-true/repo/cords-postalign/SE_tumor --local","title":"Use TPO in Local Mode"},{"location":"development/use-tpo-in-local-mode/#run-cords-analysis-locally","text":"","title":"Run Cords analysis locally"},{"location":"development/use-tpo-in-local-mode/#step1-configurate-runtime","text":"Please follow the instructions on page Installation .","title":"step1. Configurate Runtime"},{"location":"development/use-tpo-in-local-mode/#step2-modify-runtime-setting-in-config-file","text":"Please follow the instructions on page runtime-local-mode .","title":"step2. Modify runtime setting in config file"},{"location":"development/use-tpo-in-local-mode/#step3-add-local-in-running-command-examples","text":"","title":"Step3. Add --local in running command. Examples:"},{"location":"development/use-tpo-in-local-mode/#align","text":"./tpo.py -config mokefile_grch38-tpo-latest-v2-e106.ini cords_align SE_tumor SI_31729 gs://mctp-tpo/jhgong-tpo-tiny-true/tiny_mctp_SI_31729_H7MFFDSXY_2_1.fq.gz gs://mctp-tpo/jhgong-tpo-tiny-true/tiny_mctp_SI_31729_H7MFFDSXY_2_2.fq.gz --local","title":"align"},{"location":"development/use-tpo-in-local-mode/#post","text":"./tpo.py -config mokefile_grch38-tpo-latest-v2-e106.ini cords_postalign SE_tumor gs://mctp-tpo/jhgong-tpo-tiny-true/repo/cords-align/SE_tumor --local","title":"post"},{"location":"development/use-tpo-in-local-mode/#somatic","text":"./tpo.py -config mokefile_grch38-tpo-latest-v2-e106.ini cords_somatic SE_to_tumor -alnt gs://mctp-tpo/jhgong-tpo-tiny-true/repo/cords-postalign/SE_tumor --local","title":"somatic"},{"location":"notes/circos/","text":"Demo code to make circos plots from (meta)Vaults Load the vaults and make a metavault devtools::load_all('./rlibs/carat/') l <- list.files('./vaults', full.names=T) vaults <- lapply(l[5:10], readRDS) mv <- metaVault(vaults) Make and save a single plot and save pdf #one vault, saving to <vault$meta$id>.pdf circosPlot(vaults[[1]],savef=TRUE) # saves 1 pdf Setup to filter genes <- 'grch38.108.clean.gtf' settings <- system.file(sprintf(\"extdata/settings/%s.R\", 'default'), package=\"carat\") .args <- list( annotation=list( gene_model=genes ) ) cfg <- caratConfig(settings, .args) anno <- vaultAnno(cfg) Filter a single vault v <- triageVault(vaults[[1]], anno) v <- filterVault(v) Save one filtered vault #one filtered vault circosPlot(v,savef=TRUE) # saves 1 pdf Save all from the metavault to individual pdfs circosPlot(mv,savef=TRUE) # saves 5 pdfs Save all from the filtered metavault to individual pdfs #filtered metavault mvf <- triageVault(mv, anno) mvf <- filterVault(mvf) circosPlot(mvf,savef=TRUE) # saves 5 pdfs Cohort Level Circos Plots The cohortCircosPlot function provides the ability to visualize a single cohort on one plot to look for recurrent events. Description: make circos plots of the CNV, SV and Fusion data, summarizing recurrent events Usage: cohortCicosPlot( mv, bin_size = \"cytoband\", file = NULL, title = NULL, cnv = TRUE, sv = TRUE, fusion = TRUE, gnm = \"hg38\" ) Arguments: mv: a TPO dtVault bin_size: currently only supports 'cytoband' file: NULL pdf to save, if NULL will use default graphics device cnv, sv, fusion: booleans for which data types to display Default: TRUE The function operates on a dtVault like circosPlot : dtvt <- readRDS('triaged_dtVault.rds') cohortCicosPlot(dtvt, title=\"My Cohort\",file='./my_cohort_luad.pdf') The plot shows all SV in black and all fusions in gold, with the thickness of the lines proportional to the recurrence of the event in the cohort. The innermost ring shows genomic density of translocations (purple), the next ring shows deletions (blue), duplications (red) and inversions (green). The next ring shows the average copy of each cytoband in all samples (the y axis is C , the x or radial axis is genomic location, the colors are standard CNV colors, LOH: green, Gain: red, Loss: blue, Neutral: black). All cases are plotted with some alpha , so darker colors means more cases have that value (i.e. every cytoband has n points plotted where n is the number of cases in the cohort).","title":"circos"},{"location":"notes/circos/#cohort-level-circos-plots","text":"The cohortCircosPlot function provides the ability to visualize a single cohort on one plot to look for recurrent events. Description: make circos plots of the CNV, SV and Fusion data, summarizing recurrent events Usage: cohortCicosPlot( mv, bin_size = \"cytoband\", file = NULL, title = NULL, cnv = TRUE, sv = TRUE, fusion = TRUE, gnm = \"hg38\" ) Arguments: mv: a TPO dtVault bin_size: currently only supports 'cytoband' file: NULL pdf to save, if NULL will use default graphics device cnv, sv, fusion: booleans for which data types to display Default: TRUE The function operates on a dtVault like circosPlot : dtvt <- readRDS('triaged_dtVault.rds') cohortCicosPlot(dtvt, title=\"My Cohort\",file='./my_cohort_luad.pdf') The plot shows all SV in black and all fusions in gold, with the thickness of the lines proportional to the recurrence of the event in the cohort. The innermost ring shows genomic density of translocations (purple), the next ring shows deletions (blue), duplications (red) and inversions (green). The next ring shows the average copy of each cytoband in all samples (the y axis is C , the x or radial axis is genomic location, the colors are standard CNV colors, LOH: green, Gain: red, Loss: blue, Neutral: black). All cases are plotted with some alpha , so darker colors means more cases have that value (i.e. every cytoband has n points plotted where n is the number of cases in the cohort).","title":"Cohort Level Circos Plots"},{"location":"notes/examples/","text":"Here is a ~complete TPO run example: # Running DNA Tumor Alignment tpo.py -config mokefile-unit_test.ini cords_align SI_30806 SI_30806 gs://tpo-refs/v2/test/fastq/SI_30806_unittest_1.fq.gz gs://tpo-refs/v2/test/fastq/SI_30806_unittest_2.fq.gz & # Running DNA Normal Alignment tpo.py -config mokefile-unit_test.ini cords_align SI_30807 SI_30807 gs://tpo-refs/v2/test/fastq/SI_30807_unittest_1.fq.gz gs://tpo-refs/v2/test/fastq/SI_30807_unittest_2.fq.gz & # Running RNA Tumor Alignment tpo.py -config mokefile-unit_test.ini crisp_align SI_9706 SI_9706 gs://tpo-refs/v2/test/fastq/SI_9706_unittest_1.fq.gz gs://tpo-refs/v2/test/fastq/SI_9706_unittest_2.fq.gz & wait # Running DNA Tumor Postalignment tpo.py -config mokefile-unit_test.ini cords_postalign SI_30806 gs://mctp-hopkinal/unit_test/base/repo/cords-align/SI_30806 & # Running DNA Normal Postalignment tpo.py -config mokefile-unit_test.ini cords_postalign SI_30807 gs://mctp-hopkinal/unit_test/base/repo/cords-align/SI_30807 & wait # Running DNA Tumor QC tpo.py -config mokefile-unit_test.ini cords_misc SI_30806 gs://mctp-hopkinal/unit_test/base/repo/cords-postalign/SI_30806 & # Running DNA Normal QC tpo.py -config mokefile-unit_test.ini cords_misc SI_30807 gs://mctp-hopkinal/unit_test/base/repo/cords-postalign/SI_30807 & # Running RNA Quantification tpo.py -config mokefile-unit_test.ini crisp_quasr SI_9706 gs://mctp-hopkinal/unit_test/base/repo/crisp-align/SI_9706 & # Running RNA Fusion tpo.py -config mokefile-unit_test.ini crisp_codac SI_9706 gs://mctp-hopkinal/unit_test/base/repo/crisp-align/SI_9706 & wait # Running Variants tpo.py -config mokefile-unit_test.ini cords_somatic SI_30806-SI_30807 \\ -alnt gs://mctp-hopkinal/unit_test/base/repo/cords-postalign/SI_30806 \\ -alnn gs://mctp-hopkinal/unit_test/base/repo/cords-postalign/SI_30807 & # Running Structural tpo.py -config mokefile-unit_test.ini cords_structural SI_30806-SI_30807 \\ -alnt gs://mctp-hopkinal/unit_test/base/repo/cords-postalign/SI_30806 \\ -alnn gs://mctp-hopkinal/unit_test/base/repo/cords-postalign/SI_30807 & wait # Annotating Variants tpo.py -config mokefile-unit_test.ini carat_anno SI_30806-SI_30807 \\ -somatic gs://mctp-hopkinal/unit_test/base/repo/cords-somatic/SI_30806-SI_30807 \\ -structural gs://mctp-hopkinal/unit_test/base/repo/cords-structural/SI_30806-SI_30807 # Running CNV (with an override) tpo.py -config mokefile-unit_test.ini \\ -cargs 'CORDS_CNVEX_CAPTURE::/tpo/refs/grch38/capture/onco1500-v6a-targets-ucsc.bed.gz;CORDS_CNVEX_POOL::/tpo/refs/grch38/pools/cnvex-pool-onco1500.v6a-v1.pool.rds;' \\ cords_cnvex SI_30806-SI_30807 \\ -tvar gs://mctp-hopkinal/unit_test/base/repo/carat-anno/SI_30806-SI_30807 \\ -alnt gs://mctp-hopkinal/unit_test/base/repo/cords-postalign/SI_30806 \\ -alnn gs://mctp-hopkinal/unit_test/base/repo/cords-postalign/SI_30807 # Making Vault tpo.py -config mokefile-unit_test.ini carat_vault SI_30806-SI_30807-SI_9706 \\ -anno gs://mctp-hopkinal/unit_test/base/repo/carat-anno/SI_30806-SI_30807 \\ -misc_t gs://mctp-hopkinal/unit_test/base/repo/cords-misc/SI_30806 \\ -misc_n gs://mctp-hopkinal/unit_test/base/repo/cords-misc/SI_30807 \\ -cnvex gs://mctp-hopkinal/unit_test/base/repo/cords-cnvex/SI_30806-SI_30807 \\ -tquasr gs://mctp-hopkinal/unit_test/base/repo/crisp-quasr/SI_9706 \\ -codac gs://mctp-hopkinal/unit_test/base/repo/crisp-codac/SI_9706","title":"Examples"},{"location":"notes/integrative-analysis/","text":"Integrative Analyis Integrative Analysis documentation goes here.","title":"Integrative Analysis"},{"location":"notes/integrative-analysis/#integrative-analyis","text":"Integrative Analysis documentation goes here.","title":"Integrative Analyis"},{"location":"notes/misc/","text":"Miscelaneous Somatic/germline variable meaning ref - Reference allele; only 1 returned per line alt - alternative allele only 1 returned per line context - trinucleotide context of mutation sbs96 - strand converted mutation format TRANSCRIPT - assigned based on external information; prioritizes assignment to transcript with maximal potential harm EXON - assigned based on external information; prioritizes assignment to transcript with maximal potential harm IMPACT - can be low, moderate, high; protein changing variants moderate/high; typically only interested in moderate/high variants. Not fool proof as some synonymous variants are high impact but not shown as so. TERT/TP53 impacted post-hoc SIFT - algorithm output that predicts what should happen to a protein. \"mostly useless\" per marcin Warnings - not currently used HGVS_Equivalent - lists all transcripts for which mutation does the same thing AFT - allele frequency in the tumor (ADT/DFT); calculation not perfect as ADT weighted based on read quality; 2% cutoff ~minimum DPT - depth in tumor ( total coverage ) AFN - allele frequency in normal DPN - depth in normal ADT_FWD - in paired end sequencing, Forward read support; should be in equal proportion to ADT_REV; can be skewed in low quality input but in general expect at least 1 supporting read in ADT_REV ADT_REV - in paired end sequencing, Reverse read support; should be in equal proportion to ADT_FWD ; can be skewed in low quality input but in general expect at least 1 supporting read in ADT_FWD sor - strand odds ratio; sophisticated measure of ADT_FWD/ADT_REV; ideal is ~1 str - indel mutation occurs in a tandem repeat and furthers repeat str_ru - the repeating unit str_len - how many repeating units start str_diff - how many repeating units were added by variant ecnt - number of different variants considered w/in a region (exon?); measure of noisiness of variant region; ecnt>10 likely junk while ecnt ~3 likely true tlod - evidence that variant is in the tumor; IMPORTANT; core output of TNscope; higher the better nlod - evidence that variant is in ABSENT in normal; IMPORTANT; core output of TNscope; higher the better flod - (nlodf) - evidence variant is not in the normal assuming if it was it would be at same frequency in tumor xfet - fischer's exact test using ADT/ADN; probability different between tumor and normal sor - strand odds ratio mqrs - map quality rank sum; GATK parameter; compares quality of tumor reads to normal reads mqs - map quality score; average or median quality of mapped read; above 30 is good, above 27 is generally fine oxog - calculates oxidized guanines; TCGA legacy dbsnp - whether variant in dbsnp; contains germline and somatic variants both inherited and de novo. Also contains sequencing errors. Generally skewed towards germline (~95%) clinid- id of variant in clinvar; clinvar contains information on clinical impact of variants; can be germline or somatic. If pathogenic/likely pathogenic the variant should not be ignored clinvar - clinvar annotation cosmic_cnt - annotates variants relative to cosmic database; in how many cancer samples was this mutation seen; contains artifacts gnomad_af - database of healthy individuals rigorously filtered; allele frequency; high quality; higher frequency lower odds pathogenic kg_af - same thing as gnomad but based on 1000 samples pon - from genomic data commons; annotated from GDC as whether present in normals. If not blank then found by GDC rmsk_hit - repeat masker hit; is variant in repetitive region of genome; in general a nonzero value here indicates a questionable variant d/t challenge of aligning in repetitive regions Tn - proportion of tumors that have this variant; based on mioncoseq samples; identify low quality FFPE samples Nn - proportion of normals that has this variant; based on mioncoseq samples; identify low quality FFPE samples Taf90 - 90%ile allele frequency in the tumor; based on mioncoseq samples Naf90 - 90%ile allele frequency in the normal; based on mioncoseq samples","title":"Misc"},{"location":"notes/misc/#miscelaneous","text":"","title":"Miscelaneous"},{"location":"notes/misc/#somaticgermline-variable-meaning","text":"ref - Reference allele; only 1 returned per line alt - alternative allele only 1 returned per line context - trinucleotide context of mutation sbs96 - strand converted mutation format TRANSCRIPT - assigned based on external information; prioritizes assignment to transcript with maximal potential harm EXON - assigned based on external information; prioritizes assignment to transcript with maximal potential harm IMPACT - can be low, moderate, high; protein changing variants moderate/high; typically only interested in moderate/high variants. Not fool proof as some synonymous variants are high impact but not shown as so. TERT/TP53 impacted post-hoc SIFT - algorithm output that predicts what should happen to a protein. \"mostly useless\" per marcin Warnings - not currently used HGVS_Equivalent - lists all transcripts for which mutation does the same thing AFT - allele frequency in the tumor (ADT/DFT); calculation not perfect as ADT weighted based on read quality; 2% cutoff ~minimum DPT - depth in tumor ( total coverage ) AFN - allele frequency in normal DPN - depth in normal ADT_FWD - in paired end sequencing, Forward read support; should be in equal proportion to ADT_REV; can be skewed in low quality input but in general expect at least 1 supporting read in ADT_REV ADT_REV - in paired end sequencing, Reverse read support; should be in equal proportion to ADT_FWD ; can be skewed in low quality input but in general expect at least 1 supporting read in ADT_FWD sor - strand odds ratio; sophisticated measure of ADT_FWD/ADT_REV; ideal is ~1 str - indel mutation occurs in a tandem repeat and furthers repeat str_ru - the repeating unit str_len - how many repeating units start str_diff - how many repeating units were added by variant ecnt - number of different variants considered w/in a region (exon?); measure of noisiness of variant region; ecnt>10 likely junk while ecnt ~3 likely true tlod - evidence that variant is in the tumor; IMPORTANT; core output of TNscope; higher the better nlod - evidence that variant is in ABSENT in normal; IMPORTANT; core output of TNscope; higher the better flod - (nlodf) - evidence variant is not in the normal assuming if it was it would be at same frequency in tumor xfet - fischer's exact test using ADT/ADN; probability different between tumor and normal sor - strand odds ratio mqrs - map quality rank sum; GATK parameter; compares quality of tumor reads to normal reads mqs - map quality score; average or median quality of mapped read; above 30 is good, above 27 is generally fine oxog - calculates oxidized guanines; TCGA legacy dbsnp - whether variant in dbsnp; contains germline and somatic variants both inherited and de novo. Also contains sequencing errors. Generally skewed towards germline (~95%) clinid- id of variant in clinvar; clinvar contains information on clinical impact of variants; can be germline or somatic. If pathogenic/likely pathogenic the variant should not be ignored clinvar - clinvar annotation cosmic_cnt - annotates variants relative to cosmic database; in how many cancer samples was this mutation seen; contains artifacts gnomad_af - database of healthy individuals rigorously filtered; allele frequency; high quality; higher frequency lower odds pathogenic kg_af - same thing as gnomad but based on 1000 samples pon - from genomic data commons; annotated from GDC as whether present in normals. If not blank then found by GDC rmsk_hit - repeat masker hit; is variant in repetitive region of genome; in general a nonzero value here indicates a questionable variant d/t challenge of aligning in repetitive regions Tn - proportion of tumors that have this variant; based on mioncoseq samples; identify low quality FFPE samples Nn - proportion of normals that has this variant; based on mioncoseq samples; identify low quality FFPE samples Taf90 - 90%ile allele frequency in the tumor; based on mioncoseq samples Naf90 - 90%ile allele frequency in the normal; based on mioncoseq samples","title":"Somatic/germline variable meaning"},{"location":"notes/overview/","text":"Clinical Workflows https://github.com/mctp/tpo/issues/90","title":"Overview"},{"location":"notes/overview/#_1","text":"Clinical Workflows https://github.com/mctp/tpo/issues/90","title":""},{"location":"notes/use-cases/","text":"Two-stage CNVEX analysis We will run cords-cnvex twice, first in automated mode starting from BAM and VCF files to make the CNVEX output files (data, model, segmentation) and the results of model search i.e. somatic-model / germline-model . This output can be investigated in cnvex-viewer once an appropriate model is chosen the output can be updated by running cnvex a second time by using the output of the first run as input. First stage run CNVEX find candidate models Relevant contents of full.ini : ## CNVEX CNVEX_ASSEMBLY = hg38 CNVEX_SETTINGS = exome-pair CNVEX_CAPTURE = agilent-v4-targets-ucsc CNVEX_POOL = /tpo/refs/grch38/custom/cnvex-pool-agilent.v4-v3.pool.rds CNVEX_POPAF = GNOMAD_AF CNVEX_PROCESS = true CNVEX_SOMATIC = true CNVEX_SOMATICSEARCH = CNVEX_SOMATICPICK = top4 CNVEX_GERMLINE = true CNVEX_GERMLINESEARCH = --nogrid --nofine CNVEX_GERMLINEPICK = top1 First run, starting from BAM and VCF $ /mnt/share/code/tpo/tpo.py -config full.ini cords_cnvex --nowait \\ -alnt gs://mctp-ryan/mioncoseq/mCRPC/repo/cords-postalign/SI_24250_CCF6VANXX \\ -alnn gs://mctp-ryan/mioncoseq/mCRPC/repo/cords-postalign/SI_24251_CCF6VANXX \\ -tvar gs://mctp-ryan/mioncoseq/mCRPC/repo/carat-anno/MO_2438-SI_24250.SI_24251 \\ MO_2438-SI_24250.SI_24251 Second stage just produce a digest with a chosen model. We disable any process (no need this has already been done). Model can be chosen in cnvex-viewer or elswhere. ## CNVEX CNVEX_PROCESS = false CNVEX_SOMATIC = false CNVEX_GERMLINE = false And run CNVEX with the chosen model /mnt/share/code/tpo/tpo.py -config test_quick.ini cords_cnvex --nowait --overwrite \\ -cnvx gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251 \\ -gpick '0.98:1.98' \\ -spick '0.448:2.66' \\ MO_2438-SI_24250.SI_24251 Output will look like this: gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-config.txt gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-docker.txt gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-digest-00.rds <---- gpick gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-digest-01.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-err gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-model.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-segment.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-opts.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-00.rds <---- spick gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-01.rds | auto picks gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-02.rds | gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-03.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-04.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-err gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-model.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-segment.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251.rds","title":"Use Cases"},{"location":"notes/use-cases/#two-stage-cnvex-analysis","text":"We will run cords-cnvex twice, first in automated mode starting from BAM and VCF files to make the CNVEX output files (data, model, segmentation) and the results of model search i.e. somatic-model / germline-model . This output can be investigated in cnvex-viewer once an appropriate model is chosen the output can be updated by running cnvex a second time by using the output of the first run as input. First stage run CNVEX find candidate models Relevant contents of full.ini : ## CNVEX CNVEX_ASSEMBLY = hg38 CNVEX_SETTINGS = exome-pair CNVEX_CAPTURE = agilent-v4-targets-ucsc CNVEX_POOL = /tpo/refs/grch38/custom/cnvex-pool-agilent.v4-v3.pool.rds CNVEX_POPAF = GNOMAD_AF CNVEX_PROCESS = true CNVEX_SOMATIC = true CNVEX_SOMATICSEARCH = CNVEX_SOMATICPICK = top4 CNVEX_GERMLINE = true CNVEX_GERMLINESEARCH = --nogrid --nofine CNVEX_GERMLINEPICK = top1 First run, starting from BAM and VCF $ /mnt/share/code/tpo/tpo.py -config full.ini cords_cnvex --nowait \\ -alnt gs://mctp-ryan/mioncoseq/mCRPC/repo/cords-postalign/SI_24250_CCF6VANXX \\ -alnn gs://mctp-ryan/mioncoseq/mCRPC/repo/cords-postalign/SI_24251_CCF6VANXX \\ -tvar gs://mctp-ryan/mioncoseq/mCRPC/repo/carat-anno/MO_2438-SI_24250.SI_24251 \\ MO_2438-SI_24250.SI_24251 Second stage just produce a digest with a chosen model. We disable any process (no need this has already been done). Model can be chosen in cnvex-viewer or elswhere. ## CNVEX CNVEX_PROCESS = false CNVEX_SOMATIC = false CNVEX_GERMLINE = false And run CNVEX with the chosen model /mnt/share/code/tpo/tpo.py -config test_quick.ini cords_cnvex --nowait --overwrite \\ -cnvx gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251 \\ -gpick '0.98:1.98' \\ -spick '0.448:2.66' \\ MO_2438-SI_24250.SI_24251 Output will look like this: gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-config.txt gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-docker.txt gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-digest-00.rds <---- gpick gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-digest-01.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-err gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-model.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-germline-segment.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-opts.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-00.rds <---- spick gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-01.rds | auto picks gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-02.rds | gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-03.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-digest-04.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-err gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-model.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251-somatic-segment.rds gs://mctp-mcieslik-data/misc/test/repo/cords-cnvex/MO_2438-SI_24250.SI_24251/MO_2438-SI_24250.SI_24251.rds","title":"Two-stage CNVEX analysis"},{"location":"notes/variant-and-feature-manual-review/","text":"Purpose: The purpose of this page is to introduce a reliable and reproducible means of QC'ing variants and features for metavaults. Specifically this page will outline how: 1. Variants can be manually examined and re-triaged following use of the default triage functions. 2. Samples can be classified on the feature level (i.e. SPOP-mutant, WNT-altered, etc.) The functions that the code below leverages are stored primarily in /tpo/rlibs/carat/R/review.R and /tpo/rlibs/carat/R/triage.R Variant-level review Load and run default triaging # Load/triage ######### ddtmv <- qs::qread('/path/to/metavault/dtmv.rds') ddtmvf <- vaultTriage(dxv = ddtmv) ######### Export pertinent data types for feature of interest for manual review. # Export for review ######### ## choose gene g <- 'SPOP' ## compute som <- getReviewTableSomatic(ddtmvf, g) grm <- getReviewTableGermline(ddtmvf, g) str <- getReviewTableStructural(ddtmvf, g) fus <- getReviewTableFusion(ddtmvf, g) ## save write.table(som, '/save/path/spop_som.txt', row.names = F, col.names = T, quote = F, sep = '\\t') write.table(grm, '/save/path/spop_grm.txt', row.names = F, col.names = T, quote = F, sep = '\\t') write.table(str, '/save/path/TEMP/spop_str.txt', row.names = F, col.names = T, quote = F, sep = '\\t') write.table(fus, '/save/path/spop_fus.txt', row.names = F, col.names = T, quote = F, sep = '\\t') ######### Manually review externally (google sheets/excel). ** NOTE: if/when exporting structural table to google sheets, you MUST change the default format of all the cells to text. The 'default' format changes the var_ids of the structural table and mucks everything up. Load the manually review tables and combine into stacked table # Modify ######### ## load annotated tables som <- fread('/save/path/spop_reviewed_som_2023-08-10.tsv') grm <- fread('/save/path/spop_reviewed_grm_2023-08-10.tsv') str <- fread('/save/path/spop_reviewed_str_2023-08-10.tsv') fus <- fread('/save/path/spop_reviewed_fus_2023-08-10.tsv') ## make som <- formatReviewTable(som, FIELD='somatic') grm <- formatReviewTable(grm, FIELD='germline') str <- formatReviewTable(str, FIELD='structural') fus <- formatReviewTable(fus, FIELD='fusion') ## combine stacked <- rbindlist(list(som,grm,str,fus)) ## save write.table(stacked, '/save/path/spop_stacked.txt', row.names = F, col.names = T, quote = F, sep = '\\t') ######### Append the stacked table to any existing stacked tables you have. I recommend keeping a single table for all the variants you've modified. It's easy to copy paste the tables onto a single stacked table stored in google docs/excel. This can readily be used in the following step to change the vault. Then, once you've manually reviewed all the genes you want, you'll have a single stacked table which you can use to easily re-triage the variants in your vault based on your careful manual review. # Update ######### ## load stacked table TBL = fread('/save/path/stacked_variants.tsv') ## re-triage ddtmvf <- dxRescueVariants(ddtmvf, TBL) ######### Feature-level review Manually review on the feature level. This is where you can review the evidence across data types with your manually reviewed variants to make feature level calls (i.e. APC-mutant, HRD-mutant). # export ######### g_spop <- reviewFeatures(ddtmvf, g) write.table(g_spop, '/save/path/spop_gl.txt', row.names = F, col.names = T, quote = F, sep = '\\t') ######### Manually review the created table. Merge this table as you see fit with the metadata of your metavault.","title":"Variant & Feature Manual Review"},{"location":"notes/variant-and-feature-manual-review/#purpose","text":"The purpose of this page is to introduce a reliable and reproducible means of QC'ing variants and features for metavaults. Specifically this page will outline how: 1. Variants can be manually examined and re-triaged following use of the default triage functions. 2. Samples can be classified on the feature level (i.e. SPOP-mutant, WNT-altered, etc.) The functions that the code below leverages are stored primarily in /tpo/rlibs/carat/R/review.R and /tpo/rlibs/carat/R/triage.R","title":"Purpose:"},{"location":"notes/variant-and-feature-manual-review/#variant-level-review","text":"Load and run default triaging # Load/triage ######### ddtmv <- qs::qread('/path/to/metavault/dtmv.rds') ddtmvf <- vaultTriage(dxv = ddtmv) ######### Export pertinent data types for feature of interest for manual review. # Export for review ######### ## choose gene g <- 'SPOP' ## compute som <- getReviewTableSomatic(ddtmvf, g) grm <- getReviewTableGermline(ddtmvf, g) str <- getReviewTableStructural(ddtmvf, g) fus <- getReviewTableFusion(ddtmvf, g) ## save write.table(som, '/save/path/spop_som.txt', row.names = F, col.names = T, quote = F, sep = '\\t') write.table(grm, '/save/path/spop_grm.txt', row.names = F, col.names = T, quote = F, sep = '\\t') write.table(str, '/save/path/TEMP/spop_str.txt', row.names = F, col.names = T, quote = F, sep = '\\t') write.table(fus, '/save/path/spop_fus.txt', row.names = F, col.names = T, quote = F, sep = '\\t') ######### Manually review externally (google sheets/excel). ** NOTE: if/when exporting structural table to google sheets, you MUST change the default format of all the cells to text. The 'default' format changes the var_ids of the structural table and mucks everything up. Load the manually review tables and combine into stacked table # Modify ######### ## load annotated tables som <- fread('/save/path/spop_reviewed_som_2023-08-10.tsv') grm <- fread('/save/path/spop_reviewed_grm_2023-08-10.tsv') str <- fread('/save/path/spop_reviewed_str_2023-08-10.tsv') fus <- fread('/save/path/spop_reviewed_fus_2023-08-10.tsv') ## make som <- formatReviewTable(som, FIELD='somatic') grm <- formatReviewTable(grm, FIELD='germline') str <- formatReviewTable(str, FIELD='structural') fus <- formatReviewTable(fus, FIELD='fusion') ## combine stacked <- rbindlist(list(som,grm,str,fus)) ## save write.table(stacked, '/save/path/spop_stacked.txt', row.names = F, col.names = T, quote = F, sep = '\\t') ######### Append the stacked table to any existing stacked tables you have. I recommend keeping a single table for all the variants you've modified. It's easy to copy paste the tables onto a single stacked table stored in google docs/excel. This can readily be used in the following step to change the vault. Then, once you've manually reviewed all the genes you want, you'll have a single stacked table which you can use to easily re-triage the variants in your vault based on your careful manual review. # Update ######### ## load stacked table TBL = fread('/save/path/stacked_variants.tsv') ## re-triage ddtmvf <- dxRescueVariants(ddtmvf, TBL) #########","title":"Variant-level review"},{"location":"notes/variant-and-feature-manual-review/#feature-level-review","text":"Manually review on the feature level. This is where you can review the evidence across data types with your manually reviewed variants to make feature level calls (i.e. APC-mutant, HRD-mutant). # export ######### g_spop <- reviewFeatures(ddtmvf, g) write.table(g_spop, '/save/path/spop_gl.txt', row.names = F, col.names = T, quote = F, sep = '\\t') ######### Manually review the created table. Merge this table as you see fit with the metadata of your metavault.","title":"Feature-level review"},{"location":"output/silos-and-vaults/","text":"Introduction TPO Silos and Vaults solve two orthogonal problems in the storage, integration, and analysis of genomic data. By necessity genomic pipeline results are produced by a variety of tasks each saving results in a distinct file format. The CARAT packages (located in rlibs/carat ), can be thought of as TPOs ETL (Extract Transform Load) solution. It first extracts and collects all the relevant data, this data is stored in a binary Silo object. The following step creates a Vault object represents the transformed data which can be then loaded into a columnar or SQL database. Motivation The idea for dxVaults is two-fold. First, the outputs of carat-vault tend to evolve as new functionality is added and bugs being fixed. For example tables are updated with new columns (e.g. the pid , ccf , or support for normal quasr output). Vaults created at different times are not compatible for metaVault . The dtVaultUpdate function updates tables in old formats to the newest format. Second, the dtVault fixes some design decisions around legacty vaults / metaVaults allowing us to store all the data - lossless - in an SQL database, such as DuckDB, PostgreSQL and/or SQLite. This enables the use datasets that would not fit otherwise into memory, and the development of web-applications. Depending on how the data is accessed and stored (but not what data is stored) there are three types of vaults: dtVault stored using data.table and accessed either using fast data.table or slow dplyr APIs. dpVault stored using data.table and accessed using fast dtplyr APIs (i.e. dplyr for data.tables) dbVault stored using SQL and access using dbplyr APIs (i.e. dplyr for data bases) The three interfaces dplyr , dtplyr and dbplyr are very similar. In the current code base we use a mix of data.table and dplyr APIs when using legacy vaults and metaVaults . Universal functions that can work with all vaults have dx prefix. Quick Start Where is the code # dxVault* functions (these functions work on all dt/dp/db vaults) <TPO_GIT_ROOT>/rlibs/carat/R/dtVault.R # dpVault* functions <TPO_GIT_ROOT>/rlibs/carat/R/dpVault.R # dtVault* function <TPO_GIT_ROOT>/rlibs/carat/R/dtVault.R # dbVault* functions <TPO_GIT_ROOT>/rlibs/carat/R/dbVault.R # DuckDB backend <TPO_GIT_ROOT>/rlibs/carat/R/duckdb.R Create dtVaultAnno object A dtVault object is stored in a dtVault as $anno it is used in triaging. dtanno <- dtVaultAnno(\"/tpo/refs/grch38/ensembl/grch38.108.clean.gtf\") This will create a table with gene-level annotations. This is data.table which links gene_ids, gene_names, chromosomal positional information and columns indicating whether a gene is of interest or an artifact. This function allows providing optional germline and somatic genes-of-interest (GOI), and artifacts: somatic_goi , germline_goi , somatic_art . By default those lists are from corresponding files in the carat/inst/goi directory. Create a dtVault from a legacy vault vault.fn <- \".../xxx/xxx-vault.rds\" v <- readRDS(vault.fn) dtv <- dtVaultUpdate(v, dtanno) Create a multi-sample dtVault from a list of legacy vaults vault.fns <- ... vs <- lapply(vault.fns, readRDS) dtvs <- lapply(vs, dtVaultUpdate, dtanno) dtv <- dtVaultStack(dtvs) Create a dbVault from a multi-sample dtVault createDuckDBFromVault(\"test-vault-db.duckdb\", dtv) con <- dbConnect(duckdb::duckdb(), dbdir = \"test-vault-db.duckdb\", read_only = FALSE) dbv <- dbVault(con) Create a dbVault directly from multiple legacy vault files vault.fns <- ... createDuckDBFromVaultFiles(\"test-vault-db2.duckdb\", vault.fns, dtanno) Convert a dbVault into a dtVault dtv <- dtVaultCollect(dbv) Variant Triaging and Filtering Triaging and filtering of dxVaults is a two step process, giving the user the flexibility to define which tables to triage, and which how to filter variants based on the triaging result. vaultTriage applies triaging functions to the provided dxVault . This updates (optionally in-place) the triage column with a string representing the triage result i.e. PASS or as a semi-colon delimited string problemX;problemY . vaultFilter select variants that do not match a set of filtering rules. This returns a new dxVault default rules: #' list( #' somatic=\"annotation|consequence|evidence|artifact\", #' germline=\"annotation|consequence|evidence|goi\", #' structural=\"evidence\", #' fusion=\"evidence\" #' ) If the above rule-set is provided, vaultFilter will remove germline variants that with the annotation , consequence , evidence and goi problems, but structural variants will be only filtered based on evidence . ## subset dbv to only have variants related to RBM10 and convert into a dtVault dtv.rbm10 <- dbv |> dxVaultSubsetByGene(\"RBM10\") |> dbVaultCollect() ## triage the subsetted dtVault (but triaging can also be done on the complete dbVault) ## by default all tables are filtered and the dbVault or dtVault is modified in-place dtv.rbm10.t <- vaultTriage(dtv.rbm10) ## filter variants based on the default rules dtv.rbm10.f <- vaultFilter(dtv.rbm10.t) Versioning If it is helpful to keep track of changes to the structure of the dtVault object by versioning it. Each vault has a $version and only vaults with compatible versions can be stacked. The structure of a vault with a given version is stored in inst/extdata/dtvault-{ver}.schema . A new schema can be created with the following commands. the resulting schema file will be used to ensure the consistency of the object by some functions operating on dtVault objects. Here dbv is any vault. schema <- .dtVaultSchema(dtv) writeLines(\"<TPO_DIR>/rlibs/carat/inst/extdata/schema-{ver}.schema\") A schema is essentially a vault with no rows. It can be loaded as follows dtv0 <- eval(parse(\"<TPO_DIR>/rlibs/carat/inst/extdata/schema/vault-v2.schema\"))","title":"Silos and Vaults"},{"location":"output/silos-and-vaults/#introduction","text":"TPO Silos and Vaults solve two orthogonal problems in the storage, integration, and analysis of genomic data. By necessity genomic pipeline results are produced by a variety of tasks each saving results in a distinct file format. The CARAT packages (located in rlibs/carat ), can be thought of as TPOs ETL (Extract Transform Load) solution. It first extracts and collects all the relevant data, this data is stored in a binary Silo object. The following step creates a Vault object represents the transformed data which can be then loaded into a columnar or SQL database.","title":"Introduction"},{"location":"output/silos-and-vaults/#motivation","text":"The idea for dxVaults is two-fold. First, the outputs of carat-vault tend to evolve as new functionality is added and bugs being fixed. For example tables are updated with new columns (e.g. the pid , ccf , or support for normal quasr output). Vaults created at different times are not compatible for metaVault . The dtVaultUpdate function updates tables in old formats to the newest format. Second, the dtVault fixes some design decisions around legacty vaults / metaVaults allowing us to store all the data - lossless - in an SQL database, such as DuckDB, PostgreSQL and/or SQLite. This enables the use datasets that would not fit otherwise into memory, and the development of web-applications. Depending on how the data is accessed and stored (but not what data is stored) there are three types of vaults: dtVault stored using data.table and accessed either using fast data.table or slow dplyr APIs. dpVault stored using data.table and accessed using fast dtplyr APIs (i.e. dplyr for data.tables) dbVault stored using SQL and access using dbplyr APIs (i.e. dplyr for data bases) The three interfaces dplyr , dtplyr and dbplyr are very similar. In the current code base we use a mix of data.table and dplyr APIs when using legacy vaults and metaVaults . Universal functions that can work with all vaults have dx prefix.","title":"Motivation"},{"location":"output/silos-and-vaults/#quick-start","text":"Where is the code # dxVault* functions (these functions work on all dt/dp/db vaults) <TPO_GIT_ROOT>/rlibs/carat/R/dtVault.R # dpVault* functions <TPO_GIT_ROOT>/rlibs/carat/R/dpVault.R # dtVault* function <TPO_GIT_ROOT>/rlibs/carat/R/dtVault.R # dbVault* functions <TPO_GIT_ROOT>/rlibs/carat/R/dbVault.R # DuckDB backend <TPO_GIT_ROOT>/rlibs/carat/R/duckdb.R Create dtVaultAnno object A dtVault object is stored in a dtVault as $anno it is used in triaging. dtanno <- dtVaultAnno(\"/tpo/refs/grch38/ensembl/grch38.108.clean.gtf\") This will create a table with gene-level annotations. This is data.table which links gene_ids, gene_names, chromosomal positional information and columns indicating whether a gene is of interest or an artifact. This function allows providing optional germline and somatic genes-of-interest (GOI), and artifacts: somatic_goi , germline_goi , somatic_art . By default those lists are from corresponding files in the carat/inst/goi directory. Create a dtVault from a legacy vault vault.fn <- \".../xxx/xxx-vault.rds\" v <- readRDS(vault.fn) dtv <- dtVaultUpdate(v, dtanno) Create a multi-sample dtVault from a list of legacy vaults vault.fns <- ... vs <- lapply(vault.fns, readRDS) dtvs <- lapply(vs, dtVaultUpdate, dtanno) dtv <- dtVaultStack(dtvs) Create a dbVault from a multi-sample dtVault createDuckDBFromVault(\"test-vault-db.duckdb\", dtv) con <- dbConnect(duckdb::duckdb(), dbdir = \"test-vault-db.duckdb\", read_only = FALSE) dbv <- dbVault(con) Create a dbVault directly from multiple legacy vault files vault.fns <- ... createDuckDBFromVaultFiles(\"test-vault-db2.duckdb\", vault.fns, dtanno) Convert a dbVault into a dtVault dtv <- dtVaultCollect(dbv)","title":"Quick Start"},{"location":"output/silos-and-vaults/#variant-triaging-and-filtering","text":"Triaging and filtering of dxVaults is a two step process, giving the user the flexibility to define which tables to triage, and which how to filter variants based on the triaging result. vaultTriage applies triaging functions to the provided dxVault . This updates (optionally in-place) the triage column with a string representing the triage result i.e. PASS or as a semi-colon delimited string problemX;problemY . vaultFilter select variants that do not match a set of filtering rules. This returns a new dxVault default rules: #' list( #' somatic=\"annotation|consequence|evidence|artifact\", #' germline=\"annotation|consequence|evidence|goi\", #' structural=\"evidence\", #' fusion=\"evidence\" #' ) If the above rule-set is provided, vaultFilter will remove germline variants that with the annotation , consequence , evidence and goi problems, but structural variants will be only filtered based on evidence . ## subset dbv to only have variants related to RBM10 and convert into a dtVault dtv.rbm10 <- dbv |> dxVaultSubsetByGene(\"RBM10\") |> dbVaultCollect() ## triage the subsetted dtVault (but triaging can also be done on the complete dbVault) ## by default all tables are filtered and the dbVault or dtVault is modified in-place dtv.rbm10.t <- vaultTriage(dtv.rbm10) ## filter variants based on the default rules dtv.rbm10.f <- vaultFilter(dtv.rbm10.t)","title":"Variant Triaging and Filtering"},{"location":"output/silos-and-vaults/#versioning","text":"If it is helpful to keep track of changes to the structure of the dtVault object by versioning it. Each vault has a $version and only vaults with compatible versions can be stacked. The structure of a vault with a given version is stored in inst/extdata/dtvault-{ver}.schema . A new schema can be created with the following commands. the resulting schema file will be used to ensure the consistency of the object by some functions operating on dtVault objects. Here dbv is any vault. schema <- .dtVaultSchema(dtv) writeLines(\"<TPO_DIR>/rlibs/carat/inst/extdata/schema-{ver}.schema\") A schema is essentially a vault with no rows. It can be loaded as follows dtv0 <- eval(parse(\"<TPO_DIR>/rlibs/carat/inst/extdata/schema/vault-v2.schema\"))","title":"Versioning"},{"location":"output/somatic-variants/","text":"Somatic Variants The somatic variants appear in the vault under v$tables$somatic and contain the following fields Field Class Description triage character ?? id character name of analysis (from cords-somatic) var_id character chr:pos_ref/alt chr character chromosome pos integer genomic position ref character reference allele alt character alternate allele context character 3 base context for SNV sbs96 character 3 base context for SNV - sbs format gene_name character SYMBOL of gene gene_id character ENSG Gene ID TRANSCRIPT character ENST (or other) transcript ID EXON character exon #/#of exons HGVSc character DNA change HGVSp character protein change Consequence character from VEP? IMPACT character from VEP SIFT character SIFT score Warnings character ?? HGVS_Equivalent character other transcripts for which the HGVSp would be the same AFT numeric Allele Frequency Tumor AFN numeric Allele Frequence Normal ADT numeric Alt Dept Tumor DPT numeric Depth Tumor ADN numeric Alt Depth Normal DPN numeric Depth Normal ADT_FWD integer Alt depth in FWD direction ADT_REV integer Alt depth in REV direction str logical short tandem repeat? str_ru character repeat unit str_len integer repeat length str_diff integer ?? ecnt integer # of mutations in region ? tlod numeric log odds variant is in the tumor nlod numeric log odds the variant is not in the normal flod numeric log odd the variant is not in the normal given the AF in the tumor xfet numeric Fisher's Exact Test p-value ref/alt tumor/normal sor numeric Strand Odds Ratio mqrs numeric Map Quality Rank Sum mqs numeric Map Quality Sum oxog numeric likelihood of oxoG dbsnp integer dbSNP ID clinid integer CLinVar ID clinvar character ClinVar count cosmic_cnt integer COSMIC count gnomad_af numeric gnomAD Pop Allele Frequency kg_af numeric 1000G Pop Allele Frequency pon character ? rmsk_hit character RepeatMasker Hit is.indel logical is it an InDel? sblr numeric ?? all.pon integer ?? pid character ?? vcf_filter character ?? ccf numeric Percentage of cancer cells harboring this SNV m integer Number of Alleles in each tumor cell harboring this SNV","title":"Somatic Variants"},{"location":"output/somatic-variants/#somatic-variants","text":"The somatic variants appear in the vault under v$tables$somatic and contain the following fields Field Class Description triage character ?? id character name of analysis (from cords-somatic) var_id character chr:pos_ref/alt chr character chromosome pos integer genomic position ref character reference allele alt character alternate allele context character 3 base context for SNV sbs96 character 3 base context for SNV - sbs format gene_name character SYMBOL of gene gene_id character ENSG Gene ID TRANSCRIPT character ENST (or other) transcript ID EXON character exon #/#of exons HGVSc character DNA change HGVSp character protein change Consequence character from VEP? IMPACT character from VEP SIFT character SIFT score Warnings character ?? HGVS_Equivalent character other transcripts for which the HGVSp would be the same AFT numeric Allele Frequency Tumor AFN numeric Allele Frequence Normal ADT numeric Alt Dept Tumor DPT numeric Depth Tumor ADN numeric Alt Depth Normal DPN numeric Depth Normal ADT_FWD integer Alt depth in FWD direction ADT_REV integer Alt depth in REV direction str logical short tandem repeat? str_ru character repeat unit str_len integer repeat length str_diff integer ?? ecnt integer # of mutations in region ? tlod numeric log odds variant is in the tumor nlod numeric log odds the variant is not in the normal flod numeric log odd the variant is not in the normal given the AF in the tumor xfet numeric Fisher's Exact Test p-value ref/alt tumor/normal sor numeric Strand Odds Ratio mqrs numeric Map Quality Rank Sum mqs numeric Map Quality Sum oxog numeric likelihood of oxoG dbsnp integer dbSNP ID clinid integer CLinVar ID clinvar character ClinVar count cosmic_cnt integer COSMIC count gnomad_af numeric gnomAD Pop Allele Frequency kg_af numeric 1000G Pop Allele Frequency pon character ? rmsk_hit character RepeatMasker Hit is.indel logical is it an InDel? sblr numeric ?? all.pon integer ?? pid character ?? vcf_filter character ?? ccf numeric Percentage of cancer cells harboring this SNV m integer Number of Alleles in each tumor cell harboring this SNV","title":"Somatic Variants"},{"location":"setup/cloud-deployment/","text":"","title":"Cloud Deployment"},{"location":"setup/local-build/","text":"Overview This section will detail the steps necessary to make TPO operational in the GCP cloud and locally. The purpose of this process is to create two GCP images: one bootable instance image tpoboot and one reference image tporoot . Together these two GCP images are used to launch GCP instances which will execute a TPO pipeline on the cloud. Important variables: - BOOT_VER is the version of the TPO docker images and the GCP instance image - ROOT_VER is the version of the GCP image containing the TPO static and dynamic references - TPO_ROOT is the root directory of the TPO source tree - config.ini is a config file based on a copy of a template in $TPO_ROOT/config References Not all files necessary for the execution of a genomic pipeline can be kept in a git repository, such as reference genomes, gene models, large annotation files etc, custom software etc., due to their size. TPO stores such reference files in a GCP bucket. These files are used in two scenarios: when building Docker images, and when building the GCP instance image tporoot (see below). The first step in the building of TPO is to pull the reference files locally. To do this, update the config.ini file: [RUNTIME] WORK = <directory> REFS = <directory> The WORK directory is required. If WORK is provided but REFS is blank, REFS defaults to $WORK/refs , otherwise REFS should be a directory both the user and root have read-write access to. Next, execute the following command to pull references and indexes from GCP bucket to local directory: $TPO_ROOT/tpo.py -project config/my_config.ini refs_pull Depending on the internet connection this may take some time (10+ minutes). A total of 50-100GB will be downloaded. See Development for instructions on how to add custom files to the references. Docker images Once the reference files have been pulled it is possible to build the required Docker images. First, the config.ini file needs to be edited, to specify BOOT_VER a version number which will be used to tag the docker images e.g. tpobase:$BOOT_VER . The same version number will be later used to tag the GCP boot image tpoboot:$BOOT_VER (see below). [TPO] BOOT_VER = <version number> The version number above can be anything e.g. 0.0.1-rel1 . Once it is set we can build the Docker images: $TPO_ROOT/tpo.py -project config/my_config.ini boot_build This command takes two optional parameters - --nocache to build the images from scratch and drop the docker build cache. Equivalent to the --no-cache argument in docker build . - -image <image_name> to specify which image to build (among base , cords , crisp , carat ) default: all Depending on the local machine performance building the images will take up to 30min. The process will result in the creation of four images versioned by $BOOT_VER . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE gcr.io/mctp-gce/tpocarat <$BOOT_VER> 8d135b36df60 0 days ago 12GB gcr.io/mctp-gce/tpocrisp <$BOOT_VER> 603db1bf58f6 0 days ago 11.7GB gcr.io/mctp-gce/tpocords <$BOOT_VER> 1a0efa23bc63 0 days ago 11.9GB gcr.io/mctp-gce/tpobase <$BOOT_VER> f19a24294dc7 0 days ago 10.7GB Note: the IMAGE IDs and SIZEs will differ based on the time point the images are created. Google Cloud Platform (instance) image When TPO is being run on the GCP, the pipeline code is executed within a docker container which is running one of the docker images created above. Since these images are quite bulky it is not efficient to pull them (i.e. docker pull <image> ) prior to running each execution. Therefore, we will prepare a custom GCP Instance image with all of the images pre-loaded. The whole process is comprised of three steps: Push local docker images to the Google Container Registry (GCR) Pull docker images from the GCR to a GCP instance Save the updated GCP instance as a GCP image The above steps translate into two fully-automated commands: $TPO_ROOT/tpo.py -project config/my_config.ini boot_push This command will push the local images that were just built. You can go to https://console.cloud.google.com/gcr/images, then select your project from the drop-down list in the upper left corner, and see if your local docker images were successfully uploaded to GCR. $TPO_ROOT/tpo.py -project config/my_config.ini gcp_boot_build The second command will automatically perform a series of steps: start a GCP instance, pull docker images, create GCP image from instance. By default, the gcp_boot_build command will wait (issuing periodic updates) until all of those steps have been completed. This command should not be interrupted, as it will leave the build-process in a broken state. The process should not take more than 15 minutes. We can check for success by searching for the instance image on GCP: $ gcloud compute images list | grep tpoboot tpoboot-<$BOOT_VER> mctp-gce READY There are three uses for this GCP instance image: - building TPO references (covered next) - launching TPO pipelines on GCP (see: Usage ) - starting a TPO instance for debugging and development (see: Development ) Building TPO work disc Running instances require working disk space to store input, temporary, and output files. These disks are created from the work image, which is a formatted (ext4) GCP image that is automatically resized when an instance is launched. Having a pre-formatted empty image saves time when launching instances, as initializing and formatting large images takes a lot of time. $TPO_ROOT/tpo.py -project config/my_config.ini gcp_work_build Building TPO references Running TPO pipelines both locally and on the GCP requires access to a large number of reference files (e.g. reference gnomes, annotations), but even more importantly large indexes for read alignment and quantification. These indexes take many hours to build and require significant compute resources. Typically it is not possible to build those references on a standard desktop, therefore we need to build the references on the GCP. Hence unlike docker images which we build locally and push to the cloud, we build the references locally and optionally pull them locally. Building TPO references is carried out by one command gcp_refs_build , which automates the following steps: Launch a GCP instance based on the tpoboot-$BOOT_VER image (analogous to gcp_instance ) Download the static reference files (analogous to refs_pull ) Build (indexing, compilation) dynamic reference files Save the updated references as a GCP image $TPO_ROOT/tpo.py -project config/my_config.ini gcp_refs_build The above command will build a GCP instance image versioned by $ROOT_VER i.e. tporoot-$ROOT_VER . The process takes 4h or more hours to complete. We can check for success by searching for the instance image on GCP: $ gcloud compute images list | grep tporoot tporoot-<$ROOT_VER> mctp-gce READY TPO is fully operational on the GCP once tporoot-$ROOT_VER (and previously tpoboot-$BOOT_VER ) has been successfully created. To operate TPO locally the contents of tporoot-$ROOT_VER have to be downloaded locally. The process involves: creating a tar-archive of the contents of the tporoot-$ROOT_VER image pushing the tar-archive from a GCP instance to a GCP bucket pulling the tar-archive from a GCP bucket to a local instance unpacking the contents of the tar-archive The first two steps happen on the GCP cloud i.e. an instance is launched which mounts tporoot-$ROOT_VER and creates a tar-archive of its contents, this happens as part of gcp_root_push , the next two steps happen on the local instance as gcp_root_pull . $TPO_ROOT/tpo.py -project config/my_config.ini gcp_root_push $TPO_ROOT/tpo.py -project config/my_config.ini gcp_root_pull Warning: because the static and dynamic references stored in tporoot-$ROOT_VER are large, a significant amount of storage (250Gb or more) is required locally for gcp_root_pull to complete. Locally, the contents of the tporoot-$ROOT_VER image will be stored in a directory as specified by the configuration file: [RUNTIME] ROOT = <path> Here ROOT should be a directory where the user (and the UNIX root account) have both read-write access to. The whole process of pushing and pulling tporoot-$ROOT_VER locally can take several hours and will largely depend on the local network bandwidth and disk storage performance.","title":"Local Build"},{"location":"setup/local-build/#overview","text":"This section will detail the steps necessary to make TPO operational in the GCP cloud and locally. The purpose of this process is to create two GCP images: one bootable instance image tpoboot and one reference image tporoot . Together these two GCP images are used to launch GCP instances which will execute a TPO pipeline on the cloud. Important variables: - BOOT_VER is the version of the TPO docker images and the GCP instance image - ROOT_VER is the version of the GCP image containing the TPO static and dynamic references - TPO_ROOT is the root directory of the TPO source tree - config.ini is a config file based on a copy of a template in $TPO_ROOT/config","title":"Overview"},{"location":"setup/local-build/#references","text":"Not all files necessary for the execution of a genomic pipeline can be kept in a git repository, such as reference genomes, gene models, large annotation files etc, custom software etc., due to their size. TPO stores such reference files in a GCP bucket. These files are used in two scenarios: when building Docker images, and when building the GCP instance image tporoot (see below). The first step in the building of TPO is to pull the reference files locally. To do this, update the config.ini file: [RUNTIME] WORK = <directory> REFS = <directory> The WORK directory is required. If WORK is provided but REFS is blank, REFS defaults to $WORK/refs , otherwise REFS should be a directory both the user and root have read-write access to. Next, execute the following command to pull references and indexes from GCP bucket to local directory: $TPO_ROOT/tpo.py -project config/my_config.ini refs_pull Depending on the internet connection this may take some time (10+ minutes). A total of 50-100GB will be downloaded. See Development for instructions on how to add custom files to the references.","title":"References"},{"location":"setup/local-build/#docker-images","text":"Once the reference files have been pulled it is possible to build the required Docker images. First, the config.ini file needs to be edited, to specify BOOT_VER a version number which will be used to tag the docker images e.g. tpobase:$BOOT_VER . The same version number will be later used to tag the GCP boot image tpoboot:$BOOT_VER (see below). [TPO] BOOT_VER = <version number> The version number above can be anything e.g. 0.0.1-rel1 . Once it is set we can build the Docker images: $TPO_ROOT/tpo.py -project config/my_config.ini boot_build This command takes two optional parameters - --nocache to build the images from scratch and drop the docker build cache. Equivalent to the --no-cache argument in docker build . - -image <image_name> to specify which image to build (among base , cords , crisp , carat ) default: all Depending on the local machine performance building the images will take up to 30min. The process will result in the creation of four images versioned by $BOOT_VER . $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE gcr.io/mctp-gce/tpocarat <$BOOT_VER> 8d135b36df60 0 days ago 12GB gcr.io/mctp-gce/tpocrisp <$BOOT_VER> 603db1bf58f6 0 days ago 11.7GB gcr.io/mctp-gce/tpocords <$BOOT_VER> 1a0efa23bc63 0 days ago 11.9GB gcr.io/mctp-gce/tpobase <$BOOT_VER> f19a24294dc7 0 days ago 10.7GB Note: the IMAGE IDs and SIZEs will differ based on the time point the images are created.","title":"Docker images"},{"location":"setup/local-build/#google-cloud-platform-instance-image","text":"When TPO is being run on the GCP, the pipeline code is executed within a docker container which is running one of the docker images created above. Since these images are quite bulky it is not efficient to pull them (i.e. docker pull <image> ) prior to running each execution. Therefore, we will prepare a custom GCP Instance image with all of the images pre-loaded. The whole process is comprised of three steps: Push local docker images to the Google Container Registry (GCR) Pull docker images from the GCR to a GCP instance Save the updated GCP instance as a GCP image The above steps translate into two fully-automated commands: $TPO_ROOT/tpo.py -project config/my_config.ini boot_push This command will push the local images that were just built. You can go to https://console.cloud.google.com/gcr/images, then select your project from the drop-down list in the upper left corner, and see if your local docker images were successfully uploaded to GCR. $TPO_ROOT/tpo.py -project config/my_config.ini gcp_boot_build The second command will automatically perform a series of steps: start a GCP instance, pull docker images, create GCP image from instance. By default, the gcp_boot_build command will wait (issuing periodic updates) until all of those steps have been completed. This command should not be interrupted, as it will leave the build-process in a broken state. The process should not take more than 15 minutes. We can check for success by searching for the instance image on GCP: $ gcloud compute images list | grep tpoboot tpoboot-<$BOOT_VER> mctp-gce READY There are three uses for this GCP instance image: - building TPO references (covered next) - launching TPO pipelines on GCP (see: Usage ) - starting a TPO instance for debugging and development (see: Development )","title":"Google Cloud Platform (instance) image"},{"location":"setup/local-build/#building-tpo-work-disc","text":"Running instances require working disk space to store input, temporary, and output files. These disks are created from the work image, which is a formatted (ext4) GCP image that is automatically resized when an instance is launched. Having a pre-formatted empty image saves time when launching instances, as initializing and formatting large images takes a lot of time. $TPO_ROOT/tpo.py -project config/my_config.ini gcp_work_build","title":"Building TPO work disc"},{"location":"setup/local-build/#building-tpo-references","text":"Running TPO pipelines both locally and on the GCP requires access to a large number of reference files (e.g. reference gnomes, annotations), but even more importantly large indexes for read alignment and quantification. These indexes take many hours to build and require significant compute resources. Typically it is not possible to build those references on a standard desktop, therefore we need to build the references on the GCP. Hence unlike docker images which we build locally and push to the cloud, we build the references locally and optionally pull them locally. Building TPO references is carried out by one command gcp_refs_build , which automates the following steps: Launch a GCP instance based on the tpoboot-$BOOT_VER image (analogous to gcp_instance ) Download the static reference files (analogous to refs_pull ) Build (indexing, compilation) dynamic reference files Save the updated references as a GCP image $TPO_ROOT/tpo.py -project config/my_config.ini gcp_refs_build The above command will build a GCP instance image versioned by $ROOT_VER i.e. tporoot-$ROOT_VER . The process takes 4h or more hours to complete. We can check for success by searching for the instance image on GCP: $ gcloud compute images list | grep tporoot tporoot-<$ROOT_VER> mctp-gce READY TPO is fully operational on the GCP once tporoot-$ROOT_VER (and previously tpoboot-$BOOT_VER ) has been successfully created. To operate TPO locally the contents of tporoot-$ROOT_VER have to be downloaded locally. The process involves: creating a tar-archive of the contents of the tporoot-$ROOT_VER image pushing the tar-archive from a GCP instance to a GCP bucket pulling the tar-archive from a GCP bucket to a local instance unpacking the contents of the tar-archive The first two steps happen on the GCP cloud i.e. an instance is launched which mounts tporoot-$ROOT_VER and creates a tar-archive of its contents, this happens as part of gcp_root_push , the next two steps happen on the local instance as gcp_root_pull . $TPO_ROOT/tpo.py -project config/my_config.ini gcp_root_push $TPO_ROOT/tpo.py -project config/my_config.ini gcp_root_pull Warning: because the static and dynamic references stored in tporoot-$ROOT_VER are large, a significant amount of storage (250Gb or more) is required locally for gcp_root_pull to complete. Locally, the contents of the tporoot-$ROOT_VER image will be stored in a directory as specified by the configuration file: [RUNTIME] ROOT = <path> Here ROOT should be a directory where the user (and the UNIX root account) have both read-write access to. The whole process of pushing and pulling tporoot-$ROOT_VER locally can take several hours and will largely depend on the local network bandwidth and disk storage performance.","title":"Building TPO references"},{"location":"setup/local-development/","text":"Local Development When developing / testing TPO code, or analyzing TPO results it is most convenient to do it in the same runtime environment. Since all TPO are executed within a Docker container, we can Create a Docker docker for local development. The image will contain all the software and libraries used by TPO. $TPO_ROOT/tpo.py -config <config.ini> boot_build -image dev This will create a per-user versioned image with the following name $USER-tpodev:$BOOT_VER . Start a Docker container based on the development image $TPO_ROOT/tpodev.sh <config.ini> <container_name> [additional mounts] This will start a Docker container as the same user. Additional mount-points can be provided e.g. -v /my_data:/mnt/my_data . PS: 1. It's possible to detach from a container that is already running by using the CTRL + P and CTRL + Q keys. This will allow you to return to the terminal while the container continues to run in the background. 2. If you want to see a list of all running containers, you can use the docker ps command. To stop a running container, you can use the docker stop command followed by the container ID or name. 3. To attach to a running container, you can use the docker attach command followed by the container ID or <container_name> specified when started. 4. To exit the container and return to the host terminal, you can use the CTRL + D key combination.","title":"Local Development"},{"location":"setup/local-development/#local-development","text":"When developing / testing TPO code, or analyzing TPO results it is most convenient to do it in the same runtime environment. Since all TPO are executed within a Docker container, we can Create a Docker docker for local development. The image will contain all the software and libraries used by TPO. $TPO_ROOT/tpo.py -config <config.ini> boot_build -image dev This will create a per-user versioned image with the following name $USER-tpodev:$BOOT_VER . Start a Docker container based on the development image $TPO_ROOT/tpodev.sh <config.ini> <container_name> [additional mounts] This will start a Docker container as the same user. Additional mount-points can be provided e.g. -v /my_data:/mnt/my_data . PS: 1. It's possible to detach from a container that is already running by using the CTRL + P and CTRL + Q keys. This will allow you to return to the terminal while the container continues to run in the background. 2. If you want to see a list of all running containers, you can use the docker ps command. To stop a running container, you can use the docker stop command followed by the container ID or name. 3. To attach to a running container, you can use the docker attach command followed by the container ID or <container_name> specified when started. 4. To exit the container and return to the host terminal, you can use the CTRL + D key combination.","title":"Local Development"},{"location":"setup/local-installation/","text":"Prerequisites and dependencies for installation TPO is designed ground-up to work on the Google Cloud and uses Sentieon tools to accelerate and improve some analyses. Optionally, TPO can be run locally, which is recommended for development and debugging purposes, and utilize only open-source tools. This section describes the process of instaling the TPO CLI on a local machine. Additional steps are required to enable the local execution of TPO pipelines either remotely (default) or Cloud Deployment , or locally Local Deployment . Required software The following software need to be installed by the user or systems administrator: Linux (TPO is tested on Ubuntu 22.04 and RHEL8), but any modern Linux distribution should work. Python 3.6+ Moke 1.2.5+ Docker (recommended) or Podman (less well tested) Google Cloud SDK rsync, pigz All of the tools above are required for both local and remote execution of TPO. The documentation only covers installation of Moke, and basic test to veryify that Docker and the cloud SDK are correctly configured. Moke Moke provides a Python library to facilitate the writing of command-line tools, it transforms a Python module into a command-line script. Every function in the module can become a sub-command, with options inferred from the argument list and the optional doc string. The TPO CLI (Command Line Interface) is implemented as Python modules transformed at runtime by moke into a CLI. Moke can be installed from PIP. $ pip3 install --user moke If moke was installed previously it is necessary to force an upgrade. $ pip3 install --user moke -U To test the installation, try importing moke in the default Python interpreter. $ python3 >>> import moke If this succeeded please proceed to the next step. If not the following test may help identifying the problem. Moke will bind to the Python executable used during its installation. However it is helpful to check whether the site-packages directory of the default python3 , as found using the following command, matches the one for pip3 . # site-packages directory of python3 $ /usr/bin/env python3 >>> import site; site.getsitepackages() # site-packages directory of pip3 $ pip3 --version Testing the installation Docker installation and permissions can be typically verified using the following command: $ docker run hello-world | Unable to find image 'hello-world:latest' locally | latest: Pulling from library/hello-world | 2db29710123e: Pull complete | Digest: sha256:2498fce14358aa50ead0cc6c19990fc6ff866ce72aeb5546e1d59caac3d0d60f | Status: Downloaded newer image for hello-world:latest | | Hello from Docker! | This message shows that your installation appears to be working correctly. Python installation can be tested as follows: $ /usr/bin/env python | Python 3.6.9 (default, Jan 26 2021, 15:33:00) | [GCC 8.4.0] on linux | Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. The moke command run in any directory (other than the TPO codebase), should produce the following output: $ moke | moke: *** No mokefile.py found. Try 'moke new'. Stop. Google Cloud SDK Please refer to the official document of Google Cloud SDk for its installation After installation, Google Cloud SDK needs initiation with GCP account which is discussed in the next GCP account part. gcloud init If executed locally, the gcloud tool provided by the Google Cloud SDK will not be used, and gsutil will only be used to download TPO reference files and input sequencing data. Required accounts and licenses GCP account is required for TPO to run in the cloud, and Sentieon license is essential if TPO is expected to run in Sentieon mode. GCP account If intending to use TPO on the Google Cloud Platform (GCP), the user will need a GCP account with standard permissions: creating instances, images, and disks using the gcloud tool. reading from and writing to GCP buckets using the gsutil tool. (optionally) creating an GCP File Store NFS share. The custom GCP information and settings should be provided to TPO via the configuration file <config.ini> . Please refer to Configuration for details on how to configure TPO and a GCP project. Sention license Secure a license file from Sentieon . A free-trial is available. The Sentieon software license file (or license server) should be provided to TPO via the configuration file <config.ini> . Please refer here for details on how to configure TPO to use Sentieon license server or license file. Downloading the Sentieon tools is not required as these are bundled within TPO, but Sentieon is not functional without a license. Installing TPO TPO is ready directly after git clone: $ git clone https://github.com/mctp/tpo This should create a directory tpo , referred to as $TPO_ROOT throughout the documentation with the following contents $ tree --dirsfirst -L 1 tpo tpo | \u251c\u2500\u2500 base | \u251c\u2500\u2500 config | \u251c\u2500\u2500 mods | \u251c\u2500\u2500 pipe | \u251c\u2500\u2500 rlibs | \u251c\u2500\u2500 scripts | \u251c\u2500\u2500 mokefile.py | \u251c\u2500\u2500 README.md | \u2514\u2500\u2500 tpo.py -> mokefile.py To check if the installation is functional (mostly if the default python version is the same as the one used by pip3 ): $ tpo/tpo.py | usage: tpo.py [-h] [-config CONFIG] [-cargs CARGS] [-ls LS] | [-ll {debug,info,subdefault,default,warn,error}] [-lf {nltm}] | {gcp_root_build,cords_align,refs_pull,code_pull,gcp_root_push,cords_structural,gcp_root_pull,crisp_quasr,crisp_codac,crisp_germline,crisp_quant,crisp_align,template,code_push,code_build,gcp_configure,bcl,boot_pull,gcp_instance,gcp_boot_build,cords_misc,cords_germline,carat_report,cords_somatic,refs_push,boot_push,cords_unalign,cords_postalign,carat_anno,boot_build,cords_cnvex} | ... | tpo.py: error: too few arguments","title":"Local Installation"},{"location":"setup/local-installation/#prerequisites-and-dependencies-for-installation","text":"TPO is designed ground-up to work on the Google Cloud and uses Sentieon tools to accelerate and improve some analyses. Optionally, TPO can be run locally, which is recommended for development and debugging purposes, and utilize only open-source tools. This section describes the process of instaling the TPO CLI on a local machine. Additional steps are required to enable the local execution of TPO pipelines either remotely (default) or Cloud Deployment , or locally Local Deployment .","title":"Prerequisites and dependencies for installation"},{"location":"setup/local-installation/#required-software","text":"The following software need to be installed by the user or systems administrator: Linux (TPO is tested on Ubuntu 22.04 and RHEL8), but any modern Linux distribution should work. Python 3.6+ Moke 1.2.5+ Docker (recommended) or Podman (less well tested) Google Cloud SDK rsync, pigz All of the tools above are required for both local and remote execution of TPO. The documentation only covers installation of Moke, and basic test to veryify that Docker and the cloud SDK are correctly configured.","title":"Required software"},{"location":"setup/local-installation/#moke","text":"Moke provides a Python library to facilitate the writing of command-line tools, it transforms a Python module into a command-line script. Every function in the module can become a sub-command, with options inferred from the argument list and the optional doc string. The TPO CLI (Command Line Interface) is implemented as Python modules transformed at runtime by moke into a CLI. Moke can be installed from PIP. $ pip3 install --user moke If moke was installed previously it is necessary to force an upgrade. $ pip3 install --user moke -U To test the installation, try importing moke in the default Python interpreter. $ python3 >>> import moke If this succeeded please proceed to the next step. If not the following test may help identifying the problem. Moke will bind to the Python executable used during its installation. However it is helpful to check whether the site-packages directory of the default python3 , as found using the following command, matches the one for pip3 . # site-packages directory of python3 $ /usr/bin/env python3 >>> import site; site.getsitepackages() # site-packages directory of pip3 $ pip3 --version","title":"Moke"},{"location":"setup/local-installation/#testing-the-installation","text":"Docker installation and permissions can be typically verified using the following command: $ docker run hello-world | Unable to find image 'hello-world:latest' locally | latest: Pulling from library/hello-world | 2db29710123e: Pull complete | Digest: sha256:2498fce14358aa50ead0cc6c19990fc6ff866ce72aeb5546e1d59caac3d0d60f | Status: Downloaded newer image for hello-world:latest | | Hello from Docker! | This message shows that your installation appears to be working correctly. Python installation can be tested as follows: $ /usr/bin/env python | Python 3.6.9 (default, Jan 26 2021, 15:33:00) | [GCC 8.4.0] on linux | Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. The moke command run in any directory (other than the TPO codebase), should produce the following output: $ moke | moke: *** No mokefile.py found. Try 'moke new'. Stop.","title":"Testing the installation"},{"location":"setup/local-installation/#google-cloud-sdk","text":"Please refer to the official document of Google Cloud SDk for its installation After installation, Google Cloud SDK needs initiation with GCP account which is discussed in the next GCP account part. gcloud init If executed locally, the gcloud tool provided by the Google Cloud SDK will not be used, and gsutil will only be used to download TPO reference files and input sequencing data.","title":"Google Cloud SDK"},{"location":"setup/local-installation/#required-accounts-and-licenses","text":"GCP account is required for TPO to run in the cloud, and Sentieon license is essential if TPO is expected to run in Sentieon mode.","title":"Required accounts and licenses"},{"location":"setup/local-installation/#gcp-account","text":"If intending to use TPO on the Google Cloud Platform (GCP), the user will need a GCP account with standard permissions: creating instances, images, and disks using the gcloud tool. reading from and writing to GCP buckets using the gsutil tool. (optionally) creating an GCP File Store NFS share. The custom GCP information and settings should be provided to TPO via the configuration file <config.ini> . Please refer to Configuration for details on how to configure TPO and a GCP project.","title":"GCP account"},{"location":"setup/local-installation/#sention-license","text":"Secure a license file from Sentieon . A free-trial is available. The Sentieon software license file (or license server) should be provided to TPO via the configuration file <config.ini> . Please refer here for details on how to configure TPO to use Sentieon license server or license file. Downloading the Sentieon tools is not required as these are bundled within TPO, but Sentieon is not functional without a license.","title":"Sention license"},{"location":"setup/local-installation/#installing-tpo","text":"TPO is ready directly after git clone: $ git clone https://github.com/mctp/tpo This should create a directory tpo , referred to as $TPO_ROOT throughout the documentation with the following contents $ tree --dirsfirst -L 1 tpo tpo | \u251c\u2500\u2500 base | \u251c\u2500\u2500 config | \u251c\u2500\u2500 mods | \u251c\u2500\u2500 pipe | \u251c\u2500\u2500 rlibs | \u251c\u2500\u2500 scripts | \u251c\u2500\u2500 mokefile.py | \u251c\u2500\u2500 README.md | \u2514\u2500\u2500 tpo.py -> mokefile.py To check if the installation is functional (mostly if the default python version is the same as the one used by pip3 ): $ tpo/tpo.py | usage: tpo.py [-h] [-config CONFIG] [-cargs CARGS] [-ls LS] | [-ll {debug,info,subdefault,default,warn,error}] [-lf {nltm}] | {gcp_root_build,cords_align,refs_pull,code_pull,gcp_root_push,cords_structural,gcp_root_pull,crisp_quasr,crisp_codac,crisp_germline,crisp_quant,crisp_align,template,code_push,code_build,gcp_configure,bcl,boot_pull,gcp_instance,gcp_boot_build,cords_misc,cords_germline,carat_report,cords_somatic,refs_push,boot_push,cords_unalign,cords_postalign,carat_anno,boot_build,cords_cnvex} | ... | tpo.py: error: too few arguments","title":"Installing TPO"},{"location":"setup/summary/","text":"Summary Installation and configuration of TPO can be broken down into 4 steps: Local Installation . Details the installation of the TPO CLI on a local machine. Local Build . Instructions to build TPO Docker images on a local machine. Cloud Deployment . Covers the configuration and deployment of TPO on the GCP. Local Development . Provides optional instructions to enable running TPO tasks on a local machine.","title":"Summary"},{"location":"setup/summary/#summary","text":"Installation and configuration of TPO can be broken down into 4 steps: Local Installation . Details the installation of the TPO CLI on a local machine. Local Build . Instructions to build TPO Docker images on a local machine. Cloud Deployment . Covers the configuration and deployment of TPO on the GCP. Local Development . Provides optional instructions to enable running TPO tasks on a local machine.","title":"Summary"},{"location":"usage/cli/","text":"TPO Command Line Interface (CLI) TPO is controlled through a single executable shell script $TPO_ROOT/tpo.sh . This executable provide a large number of sub-commands for all major tasks of using TPO. Everything that can be done in TPO is done through this interface, including: Running all analysis tasks and pipelines. Developing, debugging, and updating TPO resources and references. Deploying and configuration locally and on the cloud. TPO CLI entrypoint The $TPO_ROOT/tpo.sh entrypoint, referred from now on as tpo.sh is, is a simple wrapper around a Python CLI $TPO_ROOT/tpo.py implemented using moke , see: Local Installation . The invocation is the same for all commands. $TPO_ROOT/tpo.sh [log_options] [tpo_options] command [command_options] [arguments...] [log_options] setting passed to moke controlling logging functions. [tpo_options] presets and ini files configuring TPO. [command_options] named parameters controlling a specific command [arguments] positional arguments to the command log_options -ls LS (file_a) [default: <stderr>] logging stream -ll {debug,info,subdefault,default,warn,error} (str) [default: default] logging level -lf {nltm} (str) [default: nltm] logging format` tpo_options - kv (``str``) list of key-value pairs `k::v` separated by `;` in single `'` quotes - custom (``str``) Custom ini file, overrides all other ini files - project (``str``) Project ini file, provides runtime and GCP settings - capture (``str``) Pick capture preset, provides capture baits and targets - settings (``str``) Pick settings preset, configures task settings - genome (``str``) Pick genome preset, configures reference genome","title":"TPO Command Line Interface"},{"location":"usage/cli/#tpo-command-line-interface-cli","text":"TPO is controlled through a single executable shell script $TPO_ROOT/tpo.sh . This executable provide a large number of sub-commands for all major tasks of using TPO. Everything that can be done in TPO is done through this interface, including: Running all analysis tasks and pipelines. Developing, debugging, and updating TPO resources and references. Deploying and configuration locally and on the cloud.","title":"TPO Command Line Interface (CLI)"},{"location":"usage/cli/#tpo-cli-entrypoint","text":"The $TPO_ROOT/tpo.sh entrypoint, referred from now on as tpo.sh is, is a simple wrapper around a Python CLI $TPO_ROOT/tpo.py implemented using moke , see: Local Installation . The invocation is the same for all commands. $TPO_ROOT/tpo.sh [log_options] [tpo_options] command [command_options] [arguments...] [log_options] setting passed to moke controlling logging functions. [tpo_options] presets and ini files configuring TPO. [command_options] named parameters controlling a specific command [arguments] positional arguments to the command log_options -ls LS (file_a) [default: <stderr>] logging stream -ll {debug,info,subdefault,default,warn,error} (str) [default: default] logging level -lf {nltm} (str) [default: nltm] logging format` tpo_options - kv (``str``) list of key-value pairs `k::v` separated by `;` in single `'` quotes - custom (``str``) Custom ini file, overrides all other ini files - project (``str``) Project ini file, provides runtime and GCP settings - capture (``str``) Pick capture preset, provides capture baits and targets - settings (``str``) Pick settings preset, configures task settings - genome (``str``) Pick genome preset, configures reference genome","title":"TPO CLI entrypoint"},{"location":"usage/configuration/bcl-barcodes/","text":"BCL Barcodes BCL barcodes documentation goes here.","title":"BCL-Barcodes"},{"location":"usage/configuration/bcl-barcodes/#bcl-barcodes","text":"BCL barcodes documentation goes here.","title":"BCL Barcodes"},{"location":"usage/configuration/genome-support/","text":"Genome Support Genome Support documentation goes here.","title":"Genome-Support"},{"location":"usage/configuration/genome-support/#genome-support","text":"Genome Support documentation goes here.","title":"Genome Support"},{"location":"usage/configuration/overview/","text":"Overview Following installation , a series of steps are required to make TPO operational. These steps involve the downloading of reference files and the creation of Docker and GCP images. Some configuration is necessary to tell TPO how to perform these steps, like where to save files (references, temporary, results), which GCP project to use, and how to authorize the Sentieon tools. In addition, the computational pipelines can be fully customized and configured, how to change their settings is described later in the Pipelines Settings section. TPO config files TPO is configured through only a single configuration file. The format of the configuration file follows the Python configparser standard, which is based on the Microsoft Windows INI format see: configparser module . The config file, typically using the .ini or .config extension is divided into several sections. Since most of the settings are required but will rarely be modified by the user, the general strategy is to start with one of the provided templates and modify it as needed. $ cp config/mokefile_grch38-tpo.template config/my_config.ini However, a few settings, denoted ~CUSTOM~ in the template files will need to be adjusted by all users. More examples of config files can be found on https://github.com/mctp/tpo-mctp-config, different versions of config files are applied in different scenarios. See how the field '~CUSTOM' are filled in those examples. This config file should now be provided to the TPO CLI using the -config parameter in order to run TPO: $ $TPO_ROOT/tpo.py -config config/my_config.ini The file sections in the config file are separated into two parts: [TPO] , [SECRETS] , [RUNTIME] , [GCP] are controlling general settings such as the details of the cloud environment, which reference files should be used; the rest is configuring computational pipelines. A single file can support both local and GCP use-cases. General settings Section Purpose [TPO] settings specify TPO versions [SECRETS] secrets such as passwords or security tokens [RUNTIME] runtime settings and locations [GCP] Google Cloud Platform Settings [TPO] Asset versioning First we need to edit the ~CUSTOM~ fields in the [TPO] section. The three parameters CODE_VER , BOOT_VER , and ROOT_VER are arbitrary strings to version GCP disk images and Docker images created and used by TPO. For example, 0.0.1-rel1 could be used and incremented whenever an update of the images is needed. ROOT_VER controls the version of the tporoot disk image on GCP. This non-executable image contains reference files and indexes to run TPO on the cloud. When running locally those contents are stored in the [RUNTIME] ROOT directory. BOOT_VER controls the version of the 4 executable TPO Docker images. CODE_VER controls the version of the TPO code Docker images. The contents of this image (i.e. the TPO code embedded within the image) are used when running any TPO pipelines on the cloud. CODE_VER serves no function when running TPO locally, in that instance the $TPO_ROOT directory cloned from git is used. An example [TPO] section could look as follows: [TPO] CODE_VER = 0.0.1-rel1 BOOT_VER = 0.0.1-rel1 ROOT_VER = 0.0.1-rel1 ```` `REFS_VER` is the location on a bucket with all the reference files which will be put into ROOT_VER ## [SECRETS] Sentieon and AWS The `SENTIEON_LICENSE` variable controls the `ip-address:port` of the Sentieon license server or a license file. Please refer to the Sentieon documentation for details on how to set up a [license server](https://support.sentieon.com/quick_start/index.html#appendix-set-up-license). If you choose to use a license file, please refer to the [installation instructions](Installation#LicenseFile). [SECRETS] SENTIEON_LICENSE = ~CUSTOM~ TPO understands the following secret variables in addition to the `SENTIEON_LICENSE`. While both variables can be stored in the config file (in the [SECRETS] section) this is not recommended as this exposes them in plain text. It is recommended to provide (in a secure way) at runtime via the `-cargs` argument to `./tpo.py`. [SECRETS] AWS_ACCESS_KEY_ID = AWS_SECRET_ACCESS_KEY = The two variables allow TPO to access files stored in AWS buckets. If provided TPO can access `s3://` read from directories like it would from `gs://`` buckets. Writing results to AWS buckets is not supported. ## [RUNTIME] Local mode The majority of settings in the `[RUNTIME]` config file section control how `TPO` uses various locations on the local computer to store various files and assets. Importantly, the user (and root) must have read-write access to all directories in this section. The optional settings `TEMP`, `RUNS`, `REFS` should be left blank to use defaults. ROOT = required directory where TPO will store a bundle of references, tools, libraries necessary for the execution of pipelines. WORK = required directory to store TPO results including TEMP, RUNS, REFS below. TEMP = optional (default: $WORK/tmp), directory to store temporary files (best to use fast SSD-backed storage) RUNS = optional (default: $WORK/runs), directory to store result files REFS = optional (default: $WORK/refs), directory to store required reference files and assets (can be slower NFS storage) ## [GCP] Remote mode Settings in the `[GCP]` and `[RUNTIME]`config section controls how TPO interacts with the GCP. Settings which control how GCP should be used: [GCP] PROJECT = LOCATION = ZONE = NETWORK = SUBNET = TAG = SERVICE_ACCOUNT = Settings which control what happens at runtime (pipeline execution): [RUNTIME] WORK_BUCKET = mctp-tpo/test-tpo WORK_DONE = what should happen if running TPO would result in overwriting, to overwrite --overwrite needs to be provided. # Pipeline configuration [BCL] BCL settings and parameters [CORDS] Settings related to DNA analyses including - Alignment - Somatic variant calling - Germline variant calling - Structural variant calling - Copy-number variant calling - Miscellaneous analyses [CRISP] Settings related to RNA analyses - Alignment - Germline variant calling - Alignment-free quantification - Alignment-based quantification - Fusion detection [CARAT] Settings related to variant annotation [TEST] Settings useful during debugging and testing ``` Version Purpose [CODE_VER] Code docker image version (default: commit id) [REFS_VER] Refs GCP image version [BOOT_VER] Boot GCP image version","title":"Overview"},{"location":"usage/configuration/overview/#overview","text":"Following installation , a series of steps are required to make TPO operational. These steps involve the downloading of reference files and the creation of Docker and GCP images. Some configuration is necessary to tell TPO how to perform these steps, like where to save files (references, temporary, results), which GCP project to use, and how to authorize the Sentieon tools. In addition, the computational pipelines can be fully customized and configured, how to change their settings is described later in the Pipelines Settings section.","title":"Overview"},{"location":"usage/configuration/overview/#tpo-config-files","text":"TPO is configured through only a single configuration file. The format of the configuration file follows the Python configparser standard, which is based on the Microsoft Windows INI format see: configparser module . The config file, typically using the .ini or .config extension is divided into several sections. Since most of the settings are required but will rarely be modified by the user, the general strategy is to start with one of the provided templates and modify it as needed. $ cp config/mokefile_grch38-tpo.template config/my_config.ini However, a few settings, denoted ~CUSTOM~ in the template files will need to be adjusted by all users. More examples of config files can be found on https://github.com/mctp/tpo-mctp-config, different versions of config files are applied in different scenarios. See how the field '~CUSTOM' are filled in those examples. This config file should now be provided to the TPO CLI using the -config parameter in order to run TPO: $ $TPO_ROOT/tpo.py -config config/my_config.ini The file sections in the config file are separated into two parts: [TPO] , [SECRETS] , [RUNTIME] , [GCP] are controlling general settings such as the details of the cloud environment, which reference files should be used; the rest is configuring computational pipelines. A single file can support both local and GCP use-cases.","title":"TPO config files"},{"location":"usage/configuration/overview/#general-settings","text":"Section Purpose [TPO] settings specify TPO versions [SECRETS] secrets such as passwords or security tokens [RUNTIME] runtime settings and locations [GCP] Google Cloud Platform Settings","title":"General settings"},{"location":"usage/configuration/overview/#tpo-asset-versioning","text":"First we need to edit the ~CUSTOM~ fields in the [TPO] section. The three parameters CODE_VER , BOOT_VER , and ROOT_VER are arbitrary strings to version GCP disk images and Docker images created and used by TPO. For example, 0.0.1-rel1 could be used and incremented whenever an update of the images is needed. ROOT_VER controls the version of the tporoot disk image on GCP. This non-executable image contains reference files and indexes to run TPO on the cloud. When running locally those contents are stored in the [RUNTIME] ROOT directory. BOOT_VER controls the version of the 4 executable TPO Docker images. CODE_VER controls the version of the TPO code Docker images. The contents of this image (i.e. the TPO code embedded within the image) are used when running any TPO pipelines on the cloud. CODE_VER serves no function when running TPO locally, in that instance the $TPO_ROOT directory cloned from git is used. An example [TPO] section could look as follows: [TPO] CODE_VER = 0.0.1-rel1 BOOT_VER = 0.0.1-rel1 ROOT_VER = 0.0.1-rel1 ```` `REFS_VER` is the location on a bucket with all the reference files which will be put into ROOT_VER ## [SECRETS] Sentieon and AWS The `SENTIEON_LICENSE` variable controls the `ip-address:port` of the Sentieon license server or a license file. Please refer to the Sentieon documentation for details on how to set up a [license server](https://support.sentieon.com/quick_start/index.html#appendix-set-up-license). If you choose to use a license file, please refer to the [installation instructions](Installation#LicenseFile). [SECRETS] SENTIEON_LICENSE = ~CUSTOM~ TPO understands the following secret variables in addition to the `SENTIEON_LICENSE`. While both variables can be stored in the config file (in the [SECRETS] section) this is not recommended as this exposes them in plain text. It is recommended to provide (in a secure way) at runtime via the `-cargs` argument to `./tpo.py`. [SECRETS] AWS_ACCESS_KEY_ID = AWS_SECRET_ACCESS_KEY = The two variables allow TPO to access files stored in AWS buckets. If provided TPO can access `s3://` read from directories like it would from `gs://`` buckets. Writing results to AWS buckets is not supported. ## [RUNTIME] Local mode The majority of settings in the `[RUNTIME]` config file section control how `TPO` uses various locations on the local computer to store various files and assets. Importantly, the user (and root) must have read-write access to all directories in this section. The optional settings `TEMP`, `RUNS`, `REFS` should be left blank to use defaults. ROOT = required directory where TPO will store a bundle of references, tools, libraries necessary for the execution of pipelines. WORK = required directory to store TPO results including TEMP, RUNS, REFS below. TEMP = optional (default: $WORK/tmp), directory to store temporary files (best to use fast SSD-backed storage) RUNS = optional (default: $WORK/runs), directory to store result files REFS = optional (default: $WORK/refs), directory to store required reference files and assets (can be slower NFS storage) ## [GCP] Remote mode Settings in the `[GCP]` and `[RUNTIME]`config section controls how TPO interacts with the GCP. Settings which control how GCP should be used: [GCP] PROJECT = LOCATION = ZONE = NETWORK = SUBNET = TAG = SERVICE_ACCOUNT = Settings which control what happens at runtime (pipeline execution): [RUNTIME] WORK_BUCKET = mctp-tpo/test-tpo WORK_DONE = what should happen if running TPO would result in overwriting, to overwrite --overwrite needs to be provided. # Pipeline configuration [BCL] BCL settings and parameters [CORDS] Settings related to DNA analyses including - Alignment - Somatic variant calling - Germline variant calling - Structural variant calling - Copy-number variant calling - Miscellaneous analyses [CRISP] Settings related to RNA analyses - Alignment - Germline variant calling - Alignment-free quantification - Alignment-based quantification - Fusion detection [CARAT] Settings related to variant annotation [TEST] Settings useful during debugging and testing ``` Version Purpose [CODE_VER] Code docker image version (default: commit id) [REFS_VER] Refs GCP image version [BOOT_VER] Boot GCP image version","title":"[TPO] Asset versioning"},{"location":"usage/configuration/settings-file/","text":"Settings Files Settings files documentation goes here.","title":"Settings"},{"location":"usage/configuration/settings-file/#settings-files","text":"Settings files documentation goes here.","title":"Settings Files"},{"location":"usage/pipelines/onco-seq/","text":"Onco Seq Onco seq documentation goes here.","title":"OncoSeq"},{"location":"usage/pipelines/onco-seq/#onco-seq","text":"Onco seq documentation goes here.","title":"Onco Seq"},{"location":"usage/pipelines/overview/","text":"Pipelines Overview Pipelines overview goes here.","title":"Overview"},{"location":"usage/pipelines/overview/#pipelines-overview","text":"Pipelines overview goes here.","title":"Pipelines Overview"},{"location":"usage/pipelines/shallow-wgs/","text":"Shallow WGS Shallow WGS documentation goes here.","title":"Shallow WGS"},{"location":"usage/pipelines/shallow-wgs/#shallow-wgs","text":"Shallow WGS documentation goes here.","title":"Shallow WGS"},{"location":"usage/pipelines/wgs-wes/","text":"WGS + WES WGS + WES documentation goes here.","title":"WGS+WES"},{"location":"usage/pipelines/wgs-wes/#wgs-wes","text":"WGS + WES documentation goes here.","title":"WGS + WES"},{"location":"usage/r-packages/caniq/","text":"caniq caniq documentation goes here.","title":"caniq"},{"location":"usage/r-packages/caniq/#caniq","text":"caniq documentation goes here.","title":"caniq"},{"location":"usage/r-packages/carat/","text":"carat carat documentation goes here.","title":"carat"},{"location":"usage/r-packages/carat/#carat","text":"carat documentation goes here.","title":"carat"},{"location":"usage/r-packages/cnvex/","text":"cnvex cnvex documentation goes here.","title":"cnvex"},{"location":"usage/r-packages/cnvex/#cnvex","text":"cnvex documentation goes here.","title":"cnvex"},{"location":"usage/r-packages/codac/","text":"codac codac documentation goes here.","title":"codac"},{"location":"usage/r-packages/codac/#codac","text":"codac documentation goes here.","title":"codac"},{"location":"usage/r-packages/overview/","text":"R Packages Overview R packages Overview documentation goes here.","title":"Overview"},{"location":"usage/r-packages/overview/#r-packages-overview","text":"R packages Overview documentation goes here.","title":"R Packages Overview"},{"location":"usage/r-packages/tpolib/","text":"tpolib tpolib documentation goes here.","title":"tpolib"},{"location":"usage/r-packages/tpolib/#tpolib","text":"tpolib documentation goes here.","title":"tpolib"},{"location":"usage/shiny-apps/cnvex-viewer/","text":"Cnvex Viewer Cnvex Viewer documentation goes here.","title":"cnvex-viewer"},{"location":"usage/shiny-apps/cnvex-viewer/#cnvex-viewer","text":"Cnvex Viewer documentation goes here.","title":"Cnvex Viewer"},{"location":"usage/shiny-apps/curve/","text":"CURVE Portal The curve R library included in TPO contains the code needed to create a case/patient-centered R Shiny portal for viewing TPO pipeline results. It enables users to deploy a Google Cloud instance with PostgeSQL and the appropriate schema, as well as the R Shiny application front end. Initialization To launch a Google Cloud instance with the appropriate schema, first ensure that CRISP_QUANT_GTF is set to the correct gene model used during the analysis (regardless of whether crisp-quant or crisp-quasr was used for the RNA analysis). This GTF will be used to create the schema for the database to store gene expression data. Additionally, set SECRETS_CURVE_USER and SECRETS_CURVE_PW to a username and password of your choice. Next, run tpo.py -config <my-config.ini> curve_db_init <name> When the task completes, a GCP instance with the <name> chosen above will be visible with gcloud compute instances list Next, copy the internal IP of the new instance and use it to create the CURVE_DB environment variable like export CURVE_DB=<ip address>:5432:curvedb:<SECRETS_CURVE_USER>:<SECRETS_CURVE_PW> Import a vault (the output of carat_vault ) into the newly created database with Rscript <TPO_ROOT>/rlibs/curve/scripts/import_vault.R -v <path to vault.rds> -g <path to gtf> After the desired vaults are imported, you must refresh some of the database views (this will happen automatically via cron @daily , but to proceed they must be updated once). <TPO_ROOT>/rlibs/curve/scripts/connect.sh # run the following SQL commands REFRESH MATERIALIZED VIEW somatic_recur; REFRESH MATERIALIZED VIEW germline_recur; REFRESH MATERIALIZED VIEW fusion_recur; Finally, to run the application in R, first set the env variable (if it is not already set) devtools::load_all('/mnt/share/code/tpo/rlibs/curve') Sys.setenv('CURVE_DB'='<ip address>:5432:curvedb:<SECRETS_CURVE_USER>:<SECRETS_CURVE_PW>') shiny::runApp('./rlibs/curve/shiny') To add additional databases to the existing GCP instance (e.g. for other projects), first set the CURVE_DB environment variable to the existing database, then modify the mokefile.ini with a new username and password (if desired), then run tpo.py -config <my-config.ini> curve_db_add <new name> Then update your CURVE_DB environment variable to the new database credentials. Adding metadata Metadata from a csv file can be added to the database and displayed in the sidebar. This metadata can either be per case (it will be applied to all analyses in the DB which match that case) or per tumor sample. The csv must have a column called case and optionally can have a column called alignid_t . If your CURVE_DB variable is set, run Rscript ./rlibs/curve/scripts/sideload_metadata.R -m ./curve_ready_metadata.csv or you can provide a database if your choosing with Rscript ./rlibs/curve/scripts/sideload_metadata.R -m ./curve_ready_metadata.csv -d \"<ip address>:5432:curvedb:<SECRETS_CURVE_USER>:<SECRETS_CURVE_PW>\" Adding feature data Feature-level data (i.e. proteomics) can be added by running Rscript ./rlibs/curve/scripts/sideload_featuredata.R -f tumor_proteomics_data.csv -g grch38.108.clean.gtf -n 'Tumor Proteomics' The -n|--name argument specifies how it will be displayed in the database, this can have an unlimited number of values (i.e. \"Tumor Proteomics\", \"Normal Proteomics\" etc.) The feature data should have a column called gene_id which maps to the gtf, and additional column names should be the tumor libraries in the groupings.alignid_t column in the database: gene_id,SI_31726,SI_31727,SI_31728,SI_31729,SI_31730,SI_31731 ENSG00000000003,25.658270426101506,25.177941588030407,26.259224491207846,25.08829842251876,25.35722453735322,24.19243418767442 ENSG00000000419,27.157933578782927,27.02590571202042,27.28431746818307,26.382172053762027,26.563799134992284,26.24792074908906 ENSG00000000457,22.107181257530307,22.377517486832268,22.786689254413982,22.247178863137258,22.519943787643896,22.711983638005982 ENSG00000000938,20.987427212477797,21.571249076966673,21.52211488625682,21.05133550391971,21.13084974148148,21.514292520849526 ENSG00000000971,29.78160643892207,29.22701562690786,29.530658892692795,29.835090234851148,30.87703665066274,30.37104840417782 ENSG00000001036,27.589627669217133,27.48563431228603,27.20853714345757,27.165062596101397,26.891624930214046,26.93188615596623 ENSG00000001084,27.564997110778258,27.621554193888702,27.385591592442797,27.116940514726977,27.752192110344744,27.97982524429814 ENSG00000001167,20.23906629527079,20.637911880139654,21.439503936347002,20.984022166587522,22.313036507648828,22.54092939525347 ENSG00000001461,NA,NA,NA,NA,NA,NA","title":"curve"},{"location":"usage/shiny-apps/curve/#curve-portal","text":"The curve R library included in TPO contains the code needed to create a case/patient-centered R Shiny portal for viewing TPO pipeline results. It enables users to deploy a Google Cloud instance with PostgeSQL and the appropriate schema, as well as the R Shiny application front end.","title":"CURVE Portal"},{"location":"usage/shiny-apps/curve/#initialization","text":"To launch a Google Cloud instance with the appropriate schema, first ensure that CRISP_QUANT_GTF is set to the correct gene model used during the analysis (regardless of whether crisp-quant or crisp-quasr was used for the RNA analysis). This GTF will be used to create the schema for the database to store gene expression data. Additionally, set SECRETS_CURVE_USER and SECRETS_CURVE_PW to a username and password of your choice. Next, run tpo.py -config <my-config.ini> curve_db_init <name> When the task completes, a GCP instance with the <name> chosen above will be visible with gcloud compute instances list Next, copy the internal IP of the new instance and use it to create the CURVE_DB environment variable like export CURVE_DB=<ip address>:5432:curvedb:<SECRETS_CURVE_USER>:<SECRETS_CURVE_PW> Import a vault (the output of carat_vault ) into the newly created database with Rscript <TPO_ROOT>/rlibs/curve/scripts/import_vault.R -v <path to vault.rds> -g <path to gtf> After the desired vaults are imported, you must refresh some of the database views (this will happen automatically via cron @daily , but to proceed they must be updated once). <TPO_ROOT>/rlibs/curve/scripts/connect.sh # run the following SQL commands REFRESH MATERIALIZED VIEW somatic_recur; REFRESH MATERIALIZED VIEW germline_recur; REFRESH MATERIALIZED VIEW fusion_recur; Finally, to run the application in R, first set the env variable (if it is not already set) devtools::load_all('/mnt/share/code/tpo/rlibs/curve') Sys.setenv('CURVE_DB'='<ip address>:5432:curvedb:<SECRETS_CURVE_USER>:<SECRETS_CURVE_PW>') shiny::runApp('./rlibs/curve/shiny') To add additional databases to the existing GCP instance (e.g. for other projects), first set the CURVE_DB environment variable to the existing database, then modify the mokefile.ini with a new username and password (if desired), then run tpo.py -config <my-config.ini> curve_db_add <new name> Then update your CURVE_DB environment variable to the new database credentials.","title":"Initialization"},{"location":"usage/shiny-apps/curve/#adding-metadata","text":"Metadata from a csv file can be added to the database and displayed in the sidebar. This metadata can either be per case (it will be applied to all analyses in the DB which match that case) or per tumor sample. The csv must have a column called case and optionally can have a column called alignid_t . If your CURVE_DB variable is set, run Rscript ./rlibs/curve/scripts/sideload_metadata.R -m ./curve_ready_metadata.csv or you can provide a database if your choosing with Rscript ./rlibs/curve/scripts/sideload_metadata.R -m ./curve_ready_metadata.csv -d \"<ip address>:5432:curvedb:<SECRETS_CURVE_USER>:<SECRETS_CURVE_PW>\"","title":"Adding metadata"},{"location":"usage/shiny-apps/curve/#adding-feature-data","text":"Feature-level data (i.e. proteomics) can be added by running Rscript ./rlibs/curve/scripts/sideload_featuredata.R -f tumor_proteomics_data.csv -g grch38.108.clean.gtf -n 'Tumor Proteomics' The -n|--name argument specifies how it will be displayed in the database, this can have an unlimited number of values (i.e. \"Tumor Proteomics\", \"Normal Proteomics\" etc.) The feature data should have a column called gene_id which maps to the gtf, and additional column names should be the tumor libraries in the groupings.alignid_t column in the database: gene_id,SI_31726,SI_31727,SI_31728,SI_31729,SI_31730,SI_31731 ENSG00000000003,25.658270426101506,25.177941588030407,26.259224491207846,25.08829842251876,25.35722453735322,24.19243418767442 ENSG00000000419,27.157933578782927,27.02590571202042,27.28431746818307,26.382172053762027,26.563799134992284,26.24792074908906 ENSG00000000457,22.107181257530307,22.377517486832268,22.786689254413982,22.247178863137258,22.519943787643896,22.711983638005982 ENSG00000000938,20.987427212477797,21.571249076966673,21.52211488625682,21.05133550391971,21.13084974148148,21.514292520849526 ENSG00000000971,29.78160643892207,29.22701562690786,29.530658892692795,29.835090234851148,30.87703665066274,30.37104840417782 ENSG00000001036,27.589627669217133,27.48563431228603,27.20853714345757,27.165062596101397,26.891624930214046,26.93188615596623 ENSG00000001084,27.564997110778258,27.621554193888702,27.385591592442797,27.116940514726977,27.752192110344744,27.97982524429814 ENSG00000001167,20.23906629527079,20.637911880139654,21.439503936347002,20.984022166587522,22.313036507648828,22.54092939525347 ENSG00000001461,NA,NA,NA,NA,NA,NA","title":"Adding feature data"},{"location":"usage/shiny-apps/overview/","text":"Shiny Apps Overview Shiny Apps overview goes here.","title":"Overview"},{"location":"usage/shiny-apps/overview/#shiny-apps-overview","text":"Shiny Apps overview goes here.","title":"Shiny Apps Overview"},{"location":"usage/shiny-apps/stuctural-viewer/","text":"Structural Viewer Structural Viewer documentation goes here.","title":"Structural Viewer"},{"location":"usage/shiny-apps/stuctural-viewer/#structural-viewer","text":"Structural Viewer documentation goes here.","title":"Structural Viewer"},{"location":"usage/tasks/analysis-tasks/","text":"Getting help from the CLI The task CLI is implemented using moke and is self-documenting. The following command can be used obtain up-to-date help for each of the tasks: tpotask.sh <task_name> -h bcl Purpose Runs Illumina bcl2fastq to generate fastq files. Inputs id Unique run identifier (e.g. \"CDMHAANXX\") tar A tar archive of the instrument's output. lib A tab delimited text file containing the Flowcell / Library / Lane / Barcode information, e.g. Flowcell Library Lane Barcode HCKNVDRXX SI_25872 1 C1[Usage](Usage) HCKNVDRXX SI_25873 1 C2 HCKNVDRXX SI_25874 1 C3 HCKNVDRXX SI_25875 1 C4 HCKNVDRXX SI_25876 1 C5 HCKNVDRXX SI_25877 1 C6 Output In $WORK_BUCKET/repo/bcl/<id> : * Renamed fastq files * config.txt containing the run parameters cords cords_align Purpose Align short-read DNA data to a reference genome. Input id Unique run identifier sample What to put in the SM of read group, important for variant calling. fq1 and fq2 The fastq files, possibly from bcl output Output In $WORK_BUCKET/repo/cords_tnscope/<id> : * <id>-aligstat.txt The alignment statistics (like Picard AlignmentSummary ) * <id>.cram The aligned file * <id>-genotype.csv and <id>-genotype.csv Results of the genotyping * other summary files (open-source version outputs four additional figures to sentieon version, i.e. meanqual.pdf , qualdist.pdf , gcbias.pdf and isize.pdf ) Note for open-source version Indel realignment is no longer supported by GATK4 since 2016 when GATK was versioned 3.6. This step has not been required by HaplotypeCaller or Mutect2 because they implement a more sophisticated and effective form of realignment. See the post . Only calculate BQSR table, but may not plot it. Example /mctp/users/rebernrj/projects/tpo/mokefile.py -config /mctp/users/rebernrj/projects/tpo-mctp-config/projects/mCRPC/v1.4/mokefile_onco1500v6a_grch38-tpo_1.4.txt cords_align SI_31707_H5L3CDRXY_1 SI_31707 gs://mctp-fastq/H5L3CDRXY/mctp_SI_31707_H5L3CDRXY_1_1.fq.gz gs://mctp-fastq/H5L3CDRXY/mctp_SI_31707_H5L3CDRXY_1_2.fq.gz --nowait --preemptible -work_disk 500G cords_unalign Purpose Create fastq files from a BAM file (lossless). Input id Unique run identifier aln Input BAM file Output cords_postalign Purpose Deduplicate, post-process and optionally combine multiple cords-align inputs into one bam file ready for variant calling and other downstream analyses. Input id Unique run identifier aln Input cords-align folders. Output Example # single aligned file /mctp/users/rebernrj/projects/tpo/mokefile.py -config /mctp/users/rebernrj/projects/tpo-mctp-config/projects/mCRPC/v1.4/mokefile_onco1500v6a_grch38-tpo_1.4.txt cords_postalign SI_31707_H5L3CDRXY gs://mctp-ryan/mioncoseq/mCRPC/repo/cords-align/SI_31707_H5L3CDRXY_1/ --nowait --preemptible # multiple aligned files /mctp/users/rebernrj/projects/tpo/mokefile.py -config /mctp/users/rebernrj/projects/tpo-mctp-config/projects/mCRPC/v1.4/mokefile_agilentv4_grch38-tpo_1.4.txt cords_postalign SI_30539 'gs://mctp-data/internal/repo/cords-align/SI_30539-HT2KGDMXX-1/;gs://mctp-data/internal/repo/cords-align/SI_30539-HT2KGDMXX-2/' --nowait --preemptible cords_misc Miscellaneous analyses for quality control and evaluation of DNA post-alignments. Purpose Input Output cords_germline Purpose Calls variants from a tumor and optional normal sample pair. Input Output cords_somatic Purpose Calls variants from a tumor and optional normal sample pair. Input Output cords_structural Purpose Calls structural variants from a tumor and optional normal sample pair. Input Output cords_cnvex Purpose Identify copy number alterations from whole-genome, whole-exome or targeted panel sequencing. Input id Unique run identifier alnn and alnt The outputs of cords-postalign for normal and tumor tvar or gvar The output of cords-somatic cnvx An existing cords-cnvex run (alternative) Output In $WORK_BUCKET/repo/cords_cnvex/<id> : * A model object which is a list of 4 objects with a complex structure. Usage The operation of cords_cnvex is controlled by multiple parameters in the TPO config file, the CNVEX settings file, and which of the inputs were provided. In general, CNVEX tries to do as much as possible with the data provided. CNVEX can do somatic (tumor) and/or germline (normal) copy-number analysis. For germline analysis a germline BAM file needs to be provided as alnn . VCF variant files can originate either from targeted sequencing (e.g. whole-exome, tvar ) or genomic sequencing ( gvar ) and can contain either one sample in tumor-only mode or two samples paired (tumor-normal) analysis mode. In tumor-only mode CNVEX can take advantage of a pool of unrelated normal samples (which needs to be constructed separately), can also be used for the analysis of the normal sample. The CNVEX algorithm consists of three stages: Input data processing. At this step the files provided in through alnt , alnn , tvar , and gvar are transformed into a cnvx object. Only the alnt input is required all other are optional. However, if no variants are provided (i.e. tvar or gvar ) only a limited analysis will be performed. Alternatively, if none of the input BAM and VCF files are provided, but a previous cnvx run is provided it will be re-processed. This allows previous CNVEX results to be updated using a newer code version or new settings. Data segmentation. At this stage the genome is being partitioned into segments of (hopefully) constant copy-number ( C ) and B-allele frequency ( BAF ). Copy-number purity/ploidy model search. This step comprises of data segmentation and model search. This process proceeds separately for the somatic (tumor) and germline (normal) component of the data. It results in multiple files being created. crisp CRISP stands for 'Clinical RNA Sequencing Interpretation Platform' it comprises of a comprehensive set of pipelines for the analysis of short-read paired-end RNA-seq data. CRISP performs three separate alignments to maximize the number of aligned chimeric reads used for fusion calling by CODAC. The first alignment, resulting in \"*alig.cram\" disables chimeric alignment all together, as a result this file only contains linearly-aligned reads. This file is used for gene expression quantification, QC, etc. The two other files disable linear alignment and perform chimeric alignment. Two chimeric files are created because STAR is run separately on overlapping read pairs (SE - long single end reads derived from overlapping paired-end reads), and non-overlapping read pairs (PE). IMPORTANT: none of the CRISP produced files contains all the reads (they are split into alig, chim-pe, chim-se, unaligned), I strongly recommend using FASTQ for long-term storage and exchange crisp_align Purpose Align paired-end short RNA-seq data to a reference genome. Input Output Outputs include: -alig.cram - Linearly aligned reads. This output results from a process that disables chimeric alignment all together, as a result this file only contains linearly-aligned reads. Index files are also created as bai and crai as well as an associated log file as *log. -chim-pe.cram - Chimeric alignment results from non-overlapping read pairs. Index files are also created as bai and crai. Index files are also created as bai and crai as well as an associated log file as *log. -chim-se.cram - Chimeric alignment results from overlapping read pairs. Index files are also created as bai and crai as well as an associated log file as *log. -unmapped_1.fq.gz - Unmapped FASTQ from direction 1. -unmapped_2.fq.gz- Unmapped FASTQ from direction 2. crisp_codac Purpose Identify Fusions from aligned RNA-seq data. Input Output crisp_quant Purpose Quantify RNA abundance using alignment-free techniques. Input Output crisp_quasr Purpose Quantify RNA abundances from short-reads using standard alignment techniques. Input Output crisp_germline Purpose Call variants in aligned RNA-seq data implementing the GATK4 strategy. Input Output carat CARAT stands for 'Comprehensive Annotation and Reporting of Aberrations in Tumors' it comprises pipelines which interpret, annotate, filter, summarize and aggregate genomic results as standardized outputs. carat_anno Purpose Annotation and interpretation of variants detected by any of the cords pipelines. Input Output carat_report Purpose Aggregation and standardized reporting of results from cords , carat , and crisp pipelines. Input Output","title":"Analysis Tasks"},{"location":"usage/tasks/analysis-tasks/#getting-help-from-the-cli","text":"The task CLI is implemented using moke and is self-documenting. The following command can be used obtain up-to-date help for each of the tasks: tpotask.sh <task_name> -h","title":"Getting help from the CLI"},{"location":"usage/tasks/analysis-tasks/#bcl","text":"","title":"bcl"},{"location":"usage/tasks/analysis-tasks/#purpose","text":"Runs Illumina bcl2fastq to generate fastq files.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#inputs","text":"id Unique run identifier (e.g. \"CDMHAANXX\") tar A tar archive of the instrument's output. lib A tab delimited text file containing the Flowcell / Library / Lane / Barcode information, e.g. Flowcell Library Lane Barcode HCKNVDRXX SI_25872 1 C1[Usage](Usage) HCKNVDRXX SI_25873 1 C2 HCKNVDRXX SI_25874 1 C3 HCKNVDRXX SI_25875 1 C4 HCKNVDRXX SI_25876 1 C5 HCKNVDRXX SI_25877 1 C6","title":"Inputs"},{"location":"usage/tasks/analysis-tasks/#output","text":"In $WORK_BUCKET/repo/bcl/<id> : * Renamed fastq files * config.txt containing the run parameters","title":"Output"},{"location":"usage/tasks/analysis-tasks/#cords","text":"","title":"cords"},{"location":"usage/tasks/analysis-tasks/#cords_align","text":"","title":"cords_align"},{"location":"usage/tasks/analysis-tasks/#purpose_1","text":"Align short-read DNA data to a reference genome.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input","text":"id Unique run identifier sample What to put in the SM of read group, important for variant calling. fq1 and fq2 The fastq files, possibly from bcl output","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_1","text":"In $WORK_BUCKET/repo/cords_tnscope/<id> : * <id>-aligstat.txt The alignment statistics (like Picard AlignmentSummary ) * <id>.cram The aligned file * <id>-genotype.csv and <id>-genotype.csv Results of the genotyping * other summary files (open-source version outputs four additional figures to sentieon version, i.e. meanqual.pdf , qualdist.pdf , gcbias.pdf and isize.pdf )","title":"Output"},{"location":"usage/tasks/analysis-tasks/#note-for-open-source-version","text":"Indel realignment is no longer supported by GATK4 since 2016 when GATK was versioned 3.6. This step has not been required by HaplotypeCaller or Mutect2 because they implement a more sophisticated and effective form of realignment. See the post . Only calculate BQSR table, but may not plot it.","title":"Note for open-source version"},{"location":"usage/tasks/analysis-tasks/#example","text":"/mctp/users/rebernrj/projects/tpo/mokefile.py -config /mctp/users/rebernrj/projects/tpo-mctp-config/projects/mCRPC/v1.4/mokefile_onco1500v6a_grch38-tpo_1.4.txt cords_align SI_31707_H5L3CDRXY_1 SI_31707 gs://mctp-fastq/H5L3CDRXY/mctp_SI_31707_H5L3CDRXY_1_1.fq.gz gs://mctp-fastq/H5L3CDRXY/mctp_SI_31707_H5L3CDRXY_1_2.fq.gz --nowait --preemptible -work_disk 500G","title":"Example"},{"location":"usage/tasks/analysis-tasks/#cords_unalign","text":"","title":"cords_unalign"},{"location":"usage/tasks/analysis-tasks/#purpose_2","text":"Create fastq files from a BAM file (lossless).","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_1","text":"id Unique run identifier aln Input BAM file","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_2","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#cords_postalign","text":"","title":"cords_postalign"},{"location":"usage/tasks/analysis-tasks/#purpose_3","text":"Deduplicate, post-process and optionally combine multiple cords-align inputs into one bam file ready for variant calling and other downstream analyses.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_2","text":"id Unique run identifier aln Input cords-align folders.","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_3","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#example_1","text":"# single aligned file /mctp/users/rebernrj/projects/tpo/mokefile.py -config /mctp/users/rebernrj/projects/tpo-mctp-config/projects/mCRPC/v1.4/mokefile_onco1500v6a_grch38-tpo_1.4.txt cords_postalign SI_31707_H5L3CDRXY gs://mctp-ryan/mioncoseq/mCRPC/repo/cords-align/SI_31707_H5L3CDRXY_1/ --nowait --preemptible # multiple aligned files /mctp/users/rebernrj/projects/tpo/mokefile.py -config /mctp/users/rebernrj/projects/tpo-mctp-config/projects/mCRPC/v1.4/mokefile_agilentv4_grch38-tpo_1.4.txt cords_postalign SI_30539 'gs://mctp-data/internal/repo/cords-align/SI_30539-HT2KGDMXX-1/;gs://mctp-data/internal/repo/cords-align/SI_30539-HT2KGDMXX-2/' --nowait --preemptible","title":"Example"},{"location":"usage/tasks/analysis-tasks/#cords_misc","text":"Miscellaneous analyses for quality control and evaluation of DNA post-alignments.","title":"cords_misc"},{"location":"usage/tasks/analysis-tasks/#purpose_4","text":"","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_3","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_4","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#cords_germline","text":"","title":"cords_germline"},{"location":"usage/tasks/analysis-tasks/#purpose_5","text":"Calls variants from a tumor and optional normal sample pair.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_4","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_5","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#cords_somatic","text":"","title":"cords_somatic"},{"location":"usage/tasks/analysis-tasks/#purpose_6","text":"Calls variants from a tumor and optional normal sample pair.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_5","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_6","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#cords_structural","text":"","title":"cords_structural"},{"location":"usage/tasks/analysis-tasks/#purpose_7","text":"Calls structural variants from a tumor and optional normal sample pair.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_6","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_7","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#cords_cnvex","text":"","title":"cords_cnvex"},{"location":"usage/tasks/analysis-tasks/#purpose_8","text":"Identify copy number alterations from whole-genome, whole-exome or targeted panel sequencing.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_7","text":"id Unique run identifier alnn and alnt The outputs of cords-postalign for normal and tumor tvar or gvar The output of cords-somatic cnvx An existing cords-cnvex run (alternative)","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_8","text":"In $WORK_BUCKET/repo/cords_cnvex/<id> : * A model object which is a list of 4 objects with a complex structure.","title":"Output"},{"location":"usage/tasks/analysis-tasks/#usage","text":"The operation of cords_cnvex is controlled by multiple parameters in the TPO config file, the CNVEX settings file, and which of the inputs were provided. In general, CNVEX tries to do as much as possible with the data provided. CNVEX can do somatic (tumor) and/or germline (normal) copy-number analysis. For germline analysis a germline BAM file needs to be provided as alnn . VCF variant files can originate either from targeted sequencing (e.g. whole-exome, tvar ) or genomic sequencing ( gvar ) and can contain either one sample in tumor-only mode or two samples paired (tumor-normal) analysis mode. In tumor-only mode CNVEX can take advantage of a pool of unrelated normal samples (which needs to be constructed separately), can also be used for the analysis of the normal sample. The CNVEX algorithm consists of three stages: Input data processing. At this step the files provided in through alnt , alnn , tvar , and gvar are transformed into a cnvx object. Only the alnt input is required all other are optional. However, if no variants are provided (i.e. tvar or gvar ) only a limited analysis will be performed. Alternatively, if none of the input BAM and VCF files are provided, but a previous cnvx run is provided it will be re-processed. This allows previous CNVEX results to be updated using a newer code version or new settings. Data segmentation. At this stage the genome is being partitioned into segments of (hopefully) constant copy-number ( C ) and B-allele frequency ( BAF ). Copy-number purity/ploidy model search. This step comprises of data segmentation and model search. This process proceeds separately for the somatic (tumor) and germline (normal) component of the data. It results in multiple files being created.","title":"Usage"},{"location":"usage/tasks/analysis-tasks/#crisp","text":"CRISP stands for 'Clinical RNA Sequencing Interpretation Platform' it comprises of a comprehensive set of pipelines for the analysis of short-read paired-end RNA-seq data. CRISP performs three separate alignments to maximize the number of aligned chimeric reads used for fusion calling by CODAC. The first alignment, resulting in \"*alig.cram\" disables chimeric alignment all together, as a result this file only contains linearly-aligned reads. This file is used for gene expression quantification, QC, etc. The two other files disable linear alignment and perform chimeric alignment. Two chimeric files are created because STAR is run separately on overlapping read pairs (SE - long single end reads derived from overlapping paired-end reads), and non-overlapping read pairs (PE). IMPORTANT: none of the CRISP produced files contains all the reads (they are split into alig, chim-pe, chim-se, unaligned), I strongly recommend using FASTQ for long-term storage and exchange","title":"crisp"},{"location":"usage/tasks/analysis-tasks/#crisp_align","text":"","title":"crisp_align"},{"location":"usage/tasks/analysis-tasks/#purpose_9","text":"Align paired-end short RNA-seq data to a reference genome.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_8","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_9","text":"Outputs include: -alig.cram - Linearly aligned reads. This output results from a process that disables chimeric alignment all together, as a result this file only contains linearly-aligned reads. Index files are also created as bai and crai as well as an associated log file as *log. -chim-pe.cram - Chimeric alignment results from non-overlapping read pairs. Index files are also created as bai and crai. Index files are also created as bai and crai as well as an associated log file as *log. -chim-se.cram - Chimeric alignment results from overlapping read pairs. Index files are also created as bai and crai as well as an associated log file as *log. -unmapped_1.fq.gz - Unmapped FASTQ from direction 1. -unmapped_2.fq.gz- Unmapped FASTQ from direction 2.","title":"Output"},{"location":"usage/tasks/analysis-tasks/#crisp_codac","text":"","title":"crisp_codac"},{"location":"usage/tasks/analysis-tasks/#purpose_10","text":"Identify Fusions from aligned RNA-seq data.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_9","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_10","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#crisp_quant","text":"","title":"crisp_quant"},{"location":"usage/tasks/analysis-tasks/#purpose_11","text":"Quantify RNA abundance using alignment-free techniques.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_10","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_11","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#crisp_quasr","text":"","title":"crisp_quasr"},{"location":"usage/tasks/analysis-tasks/#purpose_12","text":"Quantify RNA abundances from short-reads using standard alignment techniques.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_11","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_12","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#crisp_germline","text":"","title":"crisp_germline"},{"location":"usage/tasks/analysis-tasks/#purpose_13","text":"Call variants in aligned RNA-seq data implementing the GATK4 strategy.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_12","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_13","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#carat","text":"CARAT stands for 'Comprehensive Annotation and Reporting of Aberrations in Tumors' it comprises pipelines which interpret, annotate, filter, summarize and aggregate genomic results as standardized outputs.","title":"carat"},{"location":"usage/tasks/analysis-tasks/#carat_anno","text":"","title":"carat_anno"},{"location":"usage/tasks/analysis-tasks/#purpose_14","text":"Annotation and interpretation of variants detected by any of the cords pipelines.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_13","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_14","text":"","title":"Output"},{"location":"usage/tasks/analysis-tasks/#carat_report","text":"","title":"carat_report"},{"location":"usage/tasks/analysis-tasks/#purpose_15","text":"Aggregation and standardized reporting of results from cords , carat , and crisp pipelines.","title":"Purpose"},{"location":"usage/tasks/analysis-tasks/#input_14","text":"","title":"Input"},{"location":"usage/tasks/analysis-tasks/#output_15","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/","text":"Reference commands refs_pull Purpose Pull TPO static reference files from GCP bucket to local directory. Input Output refs_push Purpose Push TPO static reference files from local directory to GCP bucket. Input Output Code commands code_build Purpose Install all the necessary R libraries required for R scripts in TPO, and create a snapshot of the TPO code base including bash scripts and R scripts as a docker volume(tpocode) locally. In config file, ROOT must be set to the local code base. Input Output code_push Purpose Push code docker volume(tpocode) to the Google Container Registry (GCR) in the cloud. Input Output code_pull Purpose Pull code docker volume from the GCR. Input Output Docker images build and deployment boot_build Purpose Build TPO docker images locally. Input Output boot_push Purpose Push TPO Input Output boot_pull Purpose Input Output GCP images build and deployment gcp_boot_build Purpose Input Output gcp_root_build Purpose Input Output gcp_root_push Purpose Input Output gcp_root_pull Purpose Input Output Debugging and testing gcp_instance Purpose Input Output gcp_configure Purpose Input Output template Purpose Input Output","title":"Development Tasks"},{"location":"usage/tasks/development-tasks/#reference-commands","text":"","title":"Reference commands"},{"location":"usage/tasks/development-tasks/#refs_pull","text":"","title":"refs_pull"},{"location":"usage/tasks/development-tasks/#purpose","text":"Pull TPO static reference files from GCP bucket to local directory.","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#refs_push","text":"","title":"refs_push"},{"location":"usage/tasks/development-tasks/#purpose_1","text":"Push TPO static reference files from local directory to GCP bucket.","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_1","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_1","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#code-commands","text":"","title":"Code commands"},{"location":"usage/tasks/development-tasks/#code_build","text":"","title":"code_build"},{"location":"usage/tasks/development-tasks/#purpose_2","text":"Install all the necessary R libraries required for R scripts in TPO, and create a snapshot of the TPO code base including bash scripts and R scripts as a docker volume(tpocode) locally. In config file, ROOT must be set to the local code base.","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_2","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_2","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#code_push","text":"","title":"code_push"},{"location":"usage/tasks/development-tasks/#purpose_3","text":"Push code docker volume(tpocode) to the Google Container Registry (GCR) in the cloud.","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_3","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_3","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#code_pull","text":"","title":"code_pull"},{"location":"usage/tasks/development-tasks/#purpose_4","text":"Pull code docker volume from the GCR.","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_4","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_4","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#docker-images-build-and-deployment","text":"","title":"Docker images build and deployment"},{"location":"usage/tasks/development-tasks/#boot_build","text":"","title":"boot_build"},{"location":"usage/tasks/development-tasks/#purpose_5","text":"Build TPO docker images locally.","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_5","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_5","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#boot_push","text":"","title":"boot_push"},{"location":"usage/tasks/development-tasks/#purpose_6","text":"Push TPO","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_6","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_6","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#boot_pull","text":"","title":"boot_pull"},{"location":"usage/tasks/development-tasks/#purpose_7","text":"","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_7","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_7","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#gcp-images-build-and-deployment","text":"","title":"GCP images build and deployment"},{"location":"usage/tasks/development-tasks/#gcp_boot_build","text":"","title":"gcp_boot_build"},{"location":"usage/tasks/development-tasks/#purpose_8","text":"","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_8","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_8","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#gcp_root_build","text":"","title":"gcp_root_build"},{"location":"usage/tasks/development-tasks/#purpose_9","text":"","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_9","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_9","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#gcp_root_push","text":"","title":"gcp_root_push"},{"location":"usage/tasks/development-tasks/#purpose_10","text":"","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_10","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_10","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#gcp_root_pull","text":"","title":"gcp_root_pull"},{"location":"usage/tasks/development-tasks/#purpose_11","text":"","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_11","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_11","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#debugging-and-testing","text":"","title":"Debugging and testing"},{"location":"usage/tasks/development-tasks/#gcp_instance","text":"","title":"gcp_instance"},{"location":"usage/tasks/development-tasks/#purpose_12","text":"","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_12","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_12","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#gcp_configure","text":"","title":"gcp_configure"},{"location":"usage/tasks/development-tasks/#purpose_13","text":"","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_13","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_13","text":"","title":"Output"},{"location":"usage/tasks/development-tasks/#template","text":"","title":"template"},{"location":"usage/tasks/development-tasks/#purpose_14","text":"","title":"Purpose"},{"location":"usage/tasks/development-tasks/#input_14","text":"","title":"Input"},{"location":"usage/tasks/development-tasks/#output_14","text":"","title":"Output"},{"location":"usage/tasks/overview/","text":"Overview The TPO commands fall into two main categories pipeline commands, and infrastructure commands (see list below). The pipeline commands execute TPO analysis pipelines given some input data and save the outputs in a GCP bucket. The infrastructure commands manage references, docker / GCP images, and used mostly in the build and development process. In general, the pipeline commands are configured (via a config file) and provided with inputs (as a parameters). The infrastructure commands are also configured via the same config file, but typically do not have data inputs. Pipeline commands ## BCL bcl Execute BCL pipeline ## DNA analysis cords_align Execute CORDS ALIGN pipeline cords_unalign Execute CORDS UNALIGN pipeline cords_postalign Execute CORDS POSTALIGN pipeline cords_somatic Execute CORDS SOMATIC pipeline cords_germline Execute CORDS GERMLINE pipeline cords_cnvex Execute CORDS CNVEX pipeline cords_structural Execute CORDS STRUCTURAL pipeline cords_misc Execute CORDS MISC pipeline ## RNA analysis crisp_align Execute CRISP ALIGN pipeline crisp_quasr Execute CRISP QUASR pipeline crisp_codac Execute CRISP CODAC pipeline crisp_germline Execute CRISP GERMLINE pipeline crisp_quant Execute CRISP QUANT pipeline ## annotation and reporting carat_anno Execute CARAT ANNO pipeline carat_report Execute CARAT REPORT pipeline Infrastructure commands ## Modifying and updating references refs_pull Pull references from GCP refs_push Push references to GCP ## Code development code_build Build TPO Code Images code_push Push code image to GCR code_pull Pull code image from GCR ## Docker images build and deployment boot_build Build TPO Boot Images boot_pull Pull TPO Boot images from GCR boot_push Push TPO Boot images to GCR ## GCP images build and deployment gcp_boot_build Build boot GCP image/disk gcp_root_build Build root GCP image/disk gcp_root_push Push root/tpo from image to GS gcp_root_pull Pull root/tpo from GS to local ## Debug and testing template Execute TEMPLATE pipeline gcp_instance Start GCP instance to develop pipelines gcp_configure Configure needed GCP resources (TODO) Overview of pipeline structure Modifying pipeline command parameters Parameters used in the pipeline commands are set within the config files. Instructions for how to set these default parameters are discussed here . However, there are instances where we wish to modify pipeline commands for a subset of samples and do not wish to make an entirely new config file to do this. In this instance we can modify these paramaters by passing them when we call TPO. An axample for how to do this during Crisp-Quasr is shown below. ../../mokefile.py -config ../../mokefile_agilentv4_grch38-tpo_1.4.txt crisp_quasr SI_26995 ../../repo/crisp-align/SI_26995-CDMJCANXX-4/ --nowait --preemptible -cargs 'QUASR_MIXCR::false;QUASR_COUNTGTF::/../refs/grch38/ensembl/grch38.97.clean.gtf' In this case, we make two amendments to the config file - the first modifying the QUASR_MIXCR parameter and the second the QUSR_COUNTGTF parameter. These parameters are separated by a semicolon and encolsed with single quotes. '-cargs' is used to indicate the parameters are being passed.","title":"Overview"},{"location":"usage/tasks/overview/#overview","text":"The TPO commands fall into two main categories pipeline commands, and infrastructure commands (see list below). The pipeline commands execute TPO analysis pipelines given some input data and save the outputs in a GCP bucket. The infrastructure commands manage references, docker / GCP images, and used mostly in the build and development process. In general, the pipeline commands are configured (via a config file) and provided with inputs (as a parameters). The infrastructure commands are also configured via the same config file, but typically do not have data inputs.","title":"Overview"},{"location":"usage/tasks/overview/#pipeline-commands","text":"## BCL bcl Execute BCL pipeline ## DNA analysis cords_align Execute CORDS ALIGN pipeline cords_unalign Execute CORDS UNALIGN pipeline cords_postalign Execute CORDS POSTALIGN pipeline cords_somatic Execute CORDS SOMATIC pipeline cords_germline Execute CORDS GERMLINE pipeline cords_cnvex Execute CORDS CNVEX pipeline cords_structural Execute CORDS STRUCTURAL pipeline cords_misc Execute CORDS MISC pipeline ## RNA analysis crisp_align Execute CRISP ALIGN pipeline crisp_quasr Execute CRISP QUASR pipeline crisp_codac Execute CRISP CODAC pipeline crisp_germline Execute CRISP GERMLINE pipeline crisp_quant Execute CRISP QUANT pipeline ## annotation and reporting carat_anno Execute CARAT ANNO pipeline carat_report Execute CARAT REPORT pipeline","title":"Pipeline commands"},{"location":"usage/tasks/overview/#infrastructure-commands","text":"## Modifying and updating references refs_pull Pull references from GCP refs_push Push references to GCP ## Code development code_build Build TPO Code Images code_push Push code image to GCR code_pull Pull code image from GCR ## Docker images build and deployment boot_build Build TPO Boot Images boot_pull Pull TPO Boot images from GCR boot_push Push TPO Boot images to GCR ## GCP images build and deployment gcp_boot_build Build boot GCP image/disk gcp_root_build Build root GCP image/disk gcp_root_push Push root/tpo from image to GS gcp_root_pull Pull root/tpo from GS to local ## Debug and testing template Execute TEMPLATE pipeline gcp_instance Start GCP instance to develop pipelines gcp_configure Configure needed GCP resources (TODO)","title":"Infrastructure commands"},{"location":"usage/tasks/overview/#overview-of-pipeline-structure","text":"","title":"Overview of pipeline structure"},{"location":"usage/tasks/overview/#modifying-pipeline-command-parameters","text":"Parameters used in the pipeline commands are set within the config files. Instructions for how to set these default parameters are discussed here . However, there are instances where we wish to modify pipeline commands for a subset of samples and do not wish to make an entirely new config file to do this. In this instance we can modify these paramaters by passing them when we call TPO. An axample for how to do this during Crisp-Quasr is shown below. ../../mokefile.py -config ../../mokefile_agilentv4_grch38-tpo_1.4.txt crisp_quasr SI_26995 ../../repo/crisp-align/SI_26995-CDMJCANXX-4/ --nowait --preemptible -cargs 'QUASR_MIXCR::false;QUASR_COUNTGTF::/../refs/grch38/ensembl/grch38.97.clean.gtf' In this case, we make two amendments to the config file - the first modifying the QUASR_MIXCR parameter and the second the QUSR_COUNTGTF parameter. These parameters are separated by a semicolon and encolsed with single quotes. '-cargs' is used to indicate the parameters are being passed.","title":"Modifying pipeline command parameters"}]}